var store = [{
        "title": "API Frameworks & Transport Layers",
        "excerpt":"  Framework Independence   cache-kit is framework-agnostic. It does not:      Depend on HTTP libraries   Assume REST semantics   Tie to specific web frameworks   Make transport-level decisions   This design allows cache-kit to work seamlessly across:     REST APIs   gRPC services   GraphQL resolvers   Background workers   WebSocket servers   CLI applications   Library/SDK internals     Framework Layer vs Transport Layer   cache-kit distinguishes between framework (application structure) and transport (communication protocol).   Framework Layer   Frameworks provide application structure:     Request routing   Middleware   State management   Error handling   Transport Layer   Transports handle communication:     HTTP (REST)   gRPC (Protocol Buffers)   WebSockets   Message queues   cache-kit sits below both layers, operating on domain entities regardless of how they’re exposed.     Conceptual Separation   ┌─────────────────────────────────────────┐ │         Transport Layer                 │ │  (HTTP / gRPC / WebSocket / Workers)    │ └──────────────┬──────────────────────────┘                │ Request/Response DTOs                ↓ ┌─────────────────────────────────────────┐ │        Framework Layer                  │ │     (Axum / Actix / Tonic / Tower)      │ └──────────────┬──────────────────────────┘                │ Extract params                ↓ ┌─────────────────────────────────────────┐ │         Service Layer                   │ │     (Business logic + cache-kit)        │ └──────────────┬──────────────────────────┘                │ Domain entities                ↓ ┌─────────────────────────────────────────┐ │      Repository Layer                   │ │        (Database / ORM)                 │ └─────────────────────────────────────────┘   Key principle: Transport must never leak into cache or business logic. Cached logic should be reusable across transports.     Axum Integration (Recommended)   Axum is a modern, ergonomic web framework built on tokio and tower.   Installation   [dependencies] cache-kit = \"0.9\" axum = \"0.7\" tokio = { version = \"1.41\", features = [\"full\"] } serde = { version = \"0.9\", features = [\"derive\"] }   Complete Example   use axum::{     Router,     routing::get,     extract::{State, Path},     http::StatusCode,     response::IntoResponse,     Json, }; use cache_kit::{CacheExpander, strategy::CacheStrategy}; use cache_kit::backend::InMemoryBackend; use std::sync::Arc;  // Shared application state #[derive(Clone)] struct AppState {     cache: Arc&lt;CacheExpander&lt;InMemoryBackend&gt;&gt;,     user_repo: Arc&lt;UserRepository&gt;, }  // REST handler - clean and focused async fn get_user(     State(state): State&lt;AppState&gt;,     Path(user_id): Path&lt;String&gt;, ) -&gt; Result&lt;Json&lt;User&gt;, StatusCode&gt; {     let mut feeder = UserFeeder {         id: user_id,         user: None,     };      state.cache         .with(&amp;mut feeder, &amp;*state.user_repo, CacheStrategy::Refresh)         .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;      match feeder.user {         Some(user) =&gt; Ok(Json(user)),         None =&gt; Err(StatusCode::NOT_FOUND),     } }  async fn create_user(     State(state): State&lt;AppState&gt;,     Json(payload): Json&lt;CreateUserRequest&gt;, ) -&gt; Result&lt;Json&lt;User&gt;, StatusCode&gt; {     let user = User {         id: uuid::Uuid::new_v4().to_string(),         name: payload.name,         email: payload.email,     };      // Create in database     state.user_repo.create(&amp;user).await         .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;      // Cache the new user     let mut feeder = UserFeeder {         id: user.id.clone(),         user: Some(user.clone()),     };     state.cache         .with(&amp;mut feeder, &amp;*state.user_repo, CacheStrategy::Refresh)         .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;      Ok(Json(user)) }  #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {     // Setup cache and repository     let cache = Arc::new(CacheExpander::new(InMemoryBackend::new()));     let user_repo = Arc::new(UserRepository::new(/* db pool */));      let state = AppState { cache, user_repo };      // Build router     let app = Router::new()         .route(\"/users/:id\", get(get_user))         .route(\"/users\", axum::routing::post(create_user))         .with_state(state);      // Start server     let listener = tokio::net::TcpListener::bind(\"127.0.0.1:3000\").await?;     axum::serve(listener, app).await?;      Ok(()) }     Actix Web Integration   Actix is a mature, high-performance web framework.   See the complete actixsqlx example for:     Service layer pattern   PostgreSQL + SQLx integration   CRUD operations with caching   Docker Compose setup   Production-ready error handling   Quick Actix Example   use actix_web::{web, App, HttpResponse, HttpServer}; use cache_kit::{CacheExpander, strategy::CacheStrategy}; use cache_kit::backend::InMemoryBackend; use std::sync::Arc;  struct AppState {     cache: Arc&lt;CacheExpander&lt;InMemoryBackend&gt;&gt;,     user_repo: Arc&lt;UserRepository&gt;, }  async fn get_user(     path: web::Path&lt;String&gt;,     data: web::Data&lt;AppState&gt;, ) -&gt; HttpResponse {     let user_id = path.into_inner();      let mut feeder = UserFeeder {         id: user_id.clone(),         user: None,     };      match data.cache.with(&amp;mut feeder, &amp;*data.user_repo, CacheStrategy::Refresh) {         Ok(_) =&gt; match feeder.user {             Some(user) =&gt; HttpResponse::Ok().json(user),             None =&gt; HttpResponse::NotFound().json(serde_json::json!({                 \"error\": \"User not found\"             })),         },         Err(e) =&gt; HttpResponse::InternalServerError().json(serde_json::json!({             \"error\": format!(\"{}\", e)         })),     } }  #[actix_web::main] async fn main() -&gt; std::io::Result&lt;()&gt; {     let cache = Arc::new(CacheExpander::new(InMemoryBackend::new()));     let user_repo = Arc::new(UserRepository::new(/* db pool */));      let app_state = web::Data::new(AppState { cache, user_repo });      HttpServer::new(move || {         App::new()             .app_data(app_state.clone())             .route(\"/users/{id}\", web::get().to(get_user))     })     .bind(\"127.0.0.1:8080\")?     .run()     .await }     gRPC with Tonic   gRPC services can use cache-kit for caching database entities before serializing to Protocol Buffers.   Installation   [dependencies] cache-kit = \"0.9\" tonic = \"0.12\" prost = \"0.13\" tokio = { version = \"1.41\", features = [\"full\"] }   gRPC Service Implementation   use tonic::{Request, Response, Status}; use cache_kit::{CacheExpander, strategy::CacheStrategy}; use cache_kit::backend::RedisBackend; use std::sync::Arc;  // Generated from proto file pub mod user_service {     tonic::include_proto!(\"user\"); }  use user_service::{UserRequest, UserResponse, user_service_server::UserService};  pub struct UserServiceImpl {     cache: Arc&lt;CacheExpander&lt;RedisBackend&gt;&gt;,     repo: Arc&lt;UserRepository&gt;, }  #[tonic::async_trait] impl UserService for UserServiceImpl {     async fn get_user(         &amp;self,         request: Request&lt;UserRequest&gt;,     ) -&gt; Result&lt;Response&lt;UserResponse&gt;, Status&gt; {         let user_id = request.into_inner().id;          let mut feeder = UserFeeder {             id: user_id.clone(),             user: None,         };          // Use cache-kit to fetch cached entity         self.cache             .with(&amp;mut feeder, &amp;*self.repo, CacheStrategy::Refresh)             .map_err(|e| Status::internal(e.to_string()))?;          match feeder.user {             Some(user) =&gt; {                 // Convert domain entity to gRPC response                 let response = UserResponse {                     id: user.id,                     name: user.name,                     email: user.email,                 };                 Ok(Response::new(response))             }             None =&gt; Err(Status::not_found(\"User not found\")),         }     } }  #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {     let cache = Arc::new(CacheExpander::new(RedisBackend::new(/* config */)?));     let repo = Arc::new(UserRepository::new(/* db pool */));      let service = UserServiceImpl { cache, repo };      tonic::transport::Server::builder()         .add_service(user_service::user_service_server::UserServiceServer::new(service))         .serve(\"0.0.0.0:50051\".parse()?)         .await?;      Ok(()) }     Background Workers   cache-kit works in background workers, cron jobs, and task queues.   Example: Tokio Task   use cache_kit::{CacheExpander, strategy::CacheStrategy}; use cache_kit::backend::RedisBackend; use std::sync::Arc; use std::time::Duration;  async fn background_cache_warmer(     cache: Arc&lt;CacheExpander&lt;RedisBackend&gt;&gt;,     repo: Arc&lt;UserRepository&gt;, ) {     loop {         tokio::time::sleep(Duration::from_secs(300)).await;          // Warm cache for popular users         let popular_user_ids = vec![\"user_001\", \"user_002\", \"user_003\"];          for user_id in popular_user_ids {             let mut feeder = UserFeeder {                 id: user_id.to_string(),                 user: None,             };              if let Err(e) = cache.with(&amp;mut feeder, &amp;*repo, CacheStrategy::Refresh) {                 eprintln!(\"Cache warming error for {}: {}\", user_id, e);             }         }          println!(\"Cache warmed for popular users\");     } }  #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {     let cache = Arc::new(CacheExpander::new(RedisBackend::new(/* config */)?));     let repo = Arc::new(UserRepository::new(/* db pool */));      // Spawn background task     let cache_clone = Arc::clone(&amp;cache);     let repo_clone = Arc::clone(&amp;repo);     tokio::spawn(async move {         background_cache_warmer(cache_clone, repo_clone).await;     });      // Your main application logic     Ok(()) }     Reusable Service Layer   Define business logic once, use across transports:   pub struct UserService {     cache: Arc&lt;CacheExpander&lt;RedisBackend&gt;&gt;,     repo: Arc&lt;UserRepository&gt;, }  impl UserService {     pub fn get_user(&amp;self, id: &amp;str) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         let mut feeder = UserFeeder {             id: id.to_string(),             user: None,         };          self.cache.with(&amp;mut feeder, &amp;*self.repo, CacheStrategy::Refresh)?;         Ok(feeder.user)     }      pub async fn create_user(&amp;self, user: User) -&gt; cache_kit::Result&lt;User&gt; {         // Create logic     }      pub async fn update_user(&amp;self, user: User) -&gt; cache_kit::Result&lt;User&gt; {         // Update logic with cache invalidation     } }   Now use the same service across transports:   // REST (Axum) async fn rest_get_user(     State(service): State&lt;Arc&lt;UserService&gt;&gt;,     Path(id): Path&lt;String&gt;, ) -&gt; Result&lt;Json&lt;User&gt;, StatusCode&gt; {     service.get_user(&amp;id)         .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?         .map(Json)         .ok_or(StatusCode::NOT_FOUND) }  // gRPC (Tonic) async fn grpc_get_user(     &amp;self,     request: Request&lt;UserRequest&gt;, ) -&gt; Result&lt;Response&lt;UserResponse&gt;, Status&gt; {     let user = self.service.get_user(&amp;request.into_inner().id)         .map_err(|e| Status::internal(e.to_string()))?         .ok_or_else(|| Status::not_found(\"User not found\"))?;      Ok(Response::new(to_grpc_response(user))) }  // Background worker async fn worker_task() {     match service.get_user(\"user_001\") {         Ok(Some(user)) =&gt; process_user(user),         _ =&gt; eprintln!(\"User not found\"),     } }     API Response Caching Example   Cache API responses (not just database entities):   use cache_kit::CacheEntity; use serde::{Deserialize, Serialize};  // API response DTO #[derive(Clone, Serialize, Deserialize)] struct UserProfileResponse {     id: String,     name: String,     email: String,     followers_count: u64,     posts_count: u64, }  impl CacheEntity for UserProfileResponse {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key { self.id.clone() }     fn cache_prefix() -&gt; &amp;'static str { \"user_profile_response\" } }  // Repository fetches from database and aggregates impl DataRepository&lt;UserProfileResponse&gt; for UserProfileRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;UserProfileResponse&gt;&gt; {         // Aggregate from multiple tables         let user = self.fetch_user(id).await?;         let followers = self.count_followers(id).await?;         let posts = self.count_posts(id).await?;          Ok(Some(UserProfileResponse {             id: user.id,             name: user.name,             email: user.email,             followers_count: followers,             posts_count: posts,         }))     } }  // REST endpoint caches the full aggregated response async fn get_profile(     State(state): State&lt;AppState&gt;,     Path(user_id): Path&lt;String&gt;, ) -&gt; Result&lt;Json&lt;UserProfileResponse&gt;, StatusCode&gt; {     let mut feeder = UserProfileFeeder {         id: user_id,         response: None,     };      state.cache         .with(&amp;mut feeder, &amp;*state.profile_repo, CacheStrategy::Refresh)         .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;      match feeder.response {         Some(response) =&gt; Ok(Json(response)),         None =&gt; Err(StatusCode::NOT_FOUND),     } }     Best Practices   DO   ✅ Keep cache logic in service layer ✅ Reuse services across transports ✅ Separate DTOs from domain entities ✅ Handle cache errors gracefully at API boundary   DON’T   ❌ Put cache calls directly in HTTP handlers ❌ Leak HTTP concepts into service layer ❌ Cache transport-specific data (headers, status codes) ❌ Mix serialization formats (use domain entities, not transport DTOs)     Next Steps      Learn about Serialization formats   Explore Cache backend options   Review Design principles   See the Actix + SQLx reference implementation  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/api-frameworks/",
        "teaser": null
      },{
        "title": "Async Programming Model",
        "excerpt":"  Async-First Philosophy   cache-kit is built from the ground up as an async-first library. This design choice reflects the reality of modern Rust services where:      Database queries are async (SQLx, SeaORM, tokio-postgres)   HTTP handlers are async (Axum, Actix, warp)   gRPC services are async (tonic)   Background workers are async (tokio, async-std)   The cache layer sits between these components and must integrate seamlessly with async workflows.     Tokio Runtime Integration   cache-kit is designed for tokio-based applications. The library does not:      Spawn its own runtime   Require a specific runtime configuration   Impose threading models on your application   Instead, cache-kit operates within your existing tokio runtime.   Runtime Requirements   [dependencies] tokio = { version = \"1.41\", features = [\"rt\", \"sync\", \"macros\"] } cache-kit = \"0.9\"   The minimum required tokio features:     rt — Runtime support   sync — Synchronization primitives (Arc, Mutex, RwLock)   macros — #[tokio::main] attribute macro     Interaction Model   The typical interaction flow follows this pattern:   Async Database → Async Cache → Async Application   Example: Async Database to Async Cache   use cache_kit::{CacheEntity, CacheFeed, DataRepository, CacheExpander}; use cache_kit::backend::InMemoryBackend; use cache_kit::strategy::CacheStrategy; use sqlx::PgPool;  // Async repository using SQLx #[derive(Clone)] struct UserRepository {     pool: PgPool, }  impl DataRepository&lt;User&gt; for UserRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         let user = sqlx::query_as::&lt;_, User&gt;(             \"SELECT * FROM users WHERE id = $1\"         )         .bind(id)         .fetch_optional(&amp;self.pool)         .await         .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;          Ok(user)     } }  #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {     let pool = PgPool::connect(\"postgres://localhost/mydb\").await?;     let repo = UserRepository { pool };      let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);      let mut feeder = UserFeeder {         id: \"user_001\".to_string(),         user: None,     };      // Cache operation works seamlessly within async context     expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh).await?;      Ok(()) }     Why DataRepository is Async   The DataRepository trait uses async methods:   pub trait DataRepository&lt;T: CacheEntity&gt;: Send + Sync {     async fn fetch_by_id(&amp;self, id: &amp;T::Key) -&gt; Result&lt;Option&lt;T&gt;&gt;; }   This design is intentional and provides several benefits:   1. Native Async Support   Async trait methods align with modern Rust practices and integrate seamlessly with async databases.   2. Flexibility   You can use both sync and async database layers (see Handling Sync Code section below).   3. Backend Compatibility   Cache backends (Redis, Memcached) are inherently async, and the async trait ensures compatibility across all patterns.     Async Database Integration   The DataRepository trait is async, designed for seamless integration with modern async database drivers:   use sqlx::PgPool; use cache_kit::DataRepository;  impl DataRepository&lt;Product&gt; for ProductRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;Product&gt;&gt; {         // Direct async/await - no bridging needed         let product = sqlx::query_as!(             Product,             \"SELECT id, name, price FROM products WHERE id = $1\",             id         )         .fetch_optional(&amp;self.pool)         .await         .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;          Ok(product)     } }   Recommended async databases:     SQLx — Async, compile-time checked SQL   SeaORM — Async ORM for Rust   tokio-postgres — Pure async PostgreSQL client   ⚠️ NEVER use block_in_place + block_on   NEVER use block_in_place + Handle::current().block_on() — this pattern is incorrect. Always use async fn with .await for async databases.     Async Cache Backends   Cache backends are fully async and follow the same initialization pattern:   // Redis use cache_kit::backend::{RedisBackend, RedisConfig}; let config = RedisConfig { url: \"redis://localhost:6379\".to_string(), ..Default::default() }; let backend = RedisBackend::new(config)?;  // Memcached use cache_kit::backend::{MemcachedBackend, MemcachedConfig}; let config = MemcachedConfig { servers: vec![\"localhost:11211\".to_string()], ..Default::default() }; let backend = MemcachedBackend::new(config)?;  // InMemory (lock-free via DashMap) use cache_kit::backend::InMemoryBackend; let backend = InMemoryBackend::new();  // All use the same expander API let mut expander = CacheExpander::new(backend);   All backends work seamlessly within your async context — no special handling required.     Runtime Choice is Yours   cache-kit does not:      Require a specific tokio runtime configuration   Spawn background tasks (no tokio::spawn calls)   Create thread pools   Impose executor choices   You control:      Runtime flavor (multi-thread, current-thread)   Worker thread count   Task spawning strategy   Shutdown behavior     Sync Support (Not Recommended)   While cache-kit is async-first, you can use it in synchronous contexts if absolutely necessary by creating a runtime:   use tokio::runtime::Runtime;  fn sync_cache_operation() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {     let runtime = Runtime::new()?;      runtime.block_on(async {         let backend = InMemoryBackend::new();         let mut expander = CacheExpander::new(backend);                  expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh).await?;         Ok(())     }) }  // Or from within an existing async context: // let handle = tokio::runtime::Handle::current(); // handle.block_on(async { ... })   Important: These patterns are provided for compatibility only. Async-first design is strongly recommended for production services.     Best Practices   DO   ✅ Use tokio::main for your application entry point ✅ Make DataRepository::fetch_by_id an async function ✅ Use async database drivers (SQLx, SeaORM, tokio-postgres) ✅ Let cache-kit operate within your existing runtime ✅ Keep async boundaries explicit and clear   DON’T   ❌ Use block_in_place + block_on (incorrect pattern) ❌ Call block_on inside async contexts ❌ Create multiple tokio runtimes unnecessarily ❌ Assume cache-kit manages runtime lifecycle     Example: Full Async Service   Here’s a complete example of a tokio-based service using cache-kit:   use cache_kit::{CacheEntity, CacheFeed, DataRepository, CacheService}; use cache_kit::backend::RedisBackend; use cache_kit::strategy::CacheStrategy; use axum::{Router, routing::get, extract::State}; use sqlx::PgPool; use std::sync::Arc;  // Your entities, feeders, and repository implementations  struct AppState {     cache: CacheService&lt;RedisBackend&gt;,     repo: Arc&lt;UserRepository&gt;, }  async fn get_user(     State(state): State&lt;Arc&lt;AppState&gt;&gt;,     axum::extract::Path(user_id): axum::extract::Path&lt;String&gt;, ) -&gt; Result&lt;String, String&gt; {     let mut feeder = UserFeeder {         id: user_id,         user: None,     };      state.cache         .execute(&amp;mut feeder, &amp;*state.repo, CacheStrategy::Refresh)         .await         .map_err(|e| e.to_string())?;      Ok(format!(\"User: {:?}\", feeder.user)) }  #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {     // Database setup     let pool = PgPool::connect(\"postgres://localhost/mydb\").await?;     let repo = Arc::new(UserRepository { pool });      // Cache setup     let config = cache_kit::backend::RedisConfig::default();     let cache = CacheService::new(RedisBackend::new(config)?);      // Application state     let state = Arc::new(AppState { cache, repo });      // HTTP server (async all the way)     let app = Router::new()         .route(\"/users/:id\", get(get_user))         .with_state(state);      let listener = tokio::net::TcpListener::bind(\"127.0.0.1:3000\").await?;     axum::serve(listener, app).await?;      Ok(()) }     Next Steps      Learn about Core Concepts in cache-kit   Explore Database &amp; ORM Compatibility   Review the Actix + SQLx reference implementation  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/async-model/",
        "teaser": null
      },{
        "title": "Cache Backend Support",
        "excerpt":"  Backend Tiers   cache-kit supports multiple cache backends with explicit tiering based on production readiness and use case.                  Backend       Tier       Use Case       Persistence       Distribution                       Redis       Tier-0 (Production)       High-performance distributed cache       Optional       ✅ Yes                 Memcached       Tier-0 (Production)       Ultra-fast memory cache       ❌ No       ✅ Yes                 InMemory       Tier-1 (Dev/Test)       Local development, testing       ❌ No       ❌ No             Tier-0: Production-Grade Backends   Redis   Redis is a high-performance, feature-rich in-memory database with optional persistence.   Why Choose Redis?      ✅ Persistence — Data survives restarts (optional)   ✅ Rich data structures — Beyond key-value   ✅ Pub/Sub — Event notifications   ✅ Clustering — Horizontal scaling   ✅ Managed services — AWS ElastiCache, DigitalOcean, etc.   Installation   [dependencies] cache-kit = { version = \"0.9\", features = [\"redis\"] }   Configuration   use cache_kit::backend::{RedisBackend, RedisConfig}; use cache_kit::CacheExpander; use std::time::Duration;  let config = RedisConfig {     host: \"localhost\".to_string(),     port: 6379,     pool_size: 10,     connection_timeout: Duration::from_secs(5),     username: None,     password: None,     database: 0, };  let backend = RedisBackend::new(config)?; let expander = CacheExpander::new(backend);   Redis Configuration Options                  Field       Type       Default       Description                       host       String       \"localhost\"       Redis server hostname or IP                 port       u16       6379       Redis server port                 username       Option&lt;String&gt;       None       Redis username (Redis 6+)                 password       Option&lt;String&gt;       None       Redis password                 database       u32       0       Redis database number (0-15)                 pool_size       u32       16       Connection pool size                 connection_timeout       Duration       5s       Connection timeout           Configuration Examples   use std::time::Duration;  // Basic configuration let config = RedisConfig {     host: \"localhost\".to_string(),     port: 6379,     ..Default::default() };  // With authentication let config = RedisConfig {     host: \"example.com\".to_string(),     port: 6379,     password: Some(\"secret\".to_string()),     database: 1,     ..Default::default() };  // With custom pool size let config = RedisConfig {     host: \"localhost\".to_string(),     port: 6379,     pool_size: 32,     connection_timeout: Duration::from_secs(10),     ..Default::default() };   Managed Redis Services   cache-kit works with Redis-compatible managed services:      AWS ElastiCache for Redis     let config = RedisConfig {     host: \"my-cluster.cache.amazonaws.com\".to_string(),     port: 6379,     pool_size: 20,     ..Default::default() };           DigitalOcean Managed Redis     let config = RedisConfig {     host: \"my-redis-cluster\".to_string(),     port: 25061,     pool_size: 15,     // Note: For TLS, you may need additional configuration     ..Default::default() };           Redis Cloud     let config = RedisConfig {     host: \"redis-12345.cloud.redislabs.com\".to_string(),     port: 12345,     password: Some(\"your-password\".to_string()),     ..Default::default() };           Redis Best Practices   ✅ DO:     Use connection pooling (pool_size &gt;= expected concurrent requests)   Enable persistence for production (AOF or RDB)   Set appropriate maxmemory and eviction policies   Monitor memory usage and hit rates   Use TLS for network traffic   ❌ DON’T:     Use a single connection for high concurrency   Ignore Redis memory limits   Store unbounded data without TTLs   Skip authentication in production     Memcached   Memcached is an ultra-fast, distributed memory object caching system.   Why Choose Memcached?      ✅ Extremely fast — Optimized for speed   ✅ Distributed — Multi-server deployment   ✅ Simple — Minimal configuration   ✅ Mature — Battle-tested in production   ⚠️ Caveats:     ❌ No persistence — Data lost on restart   ❌ No wildcard deletes — Cannot delete by pattern   ❌ No pub/sub — No event notifications   Installation   [dependencies] cache-kit = { version = \"0.9\", features = [\"memcached\"] }   Configuration   use cache_kit::backend::{MemcachedBackend, MemcachedConfig}; use cache_kit::CacheExpander;  let config = MemcachedConfig {     servers: vec![\"localhost:11211\".to_string()],     max_connections: 10,     min_connections: 2, };  let backend = MemcachedBackend::new(config)?; let expander = CacheExpander::new(backend);   Memcached Configuration Options                  Field       Type       Default       Description                       servers       Vec&lt;String&gt;       Required       List of Memcached server addresses                 max_connections       usize       10       Maximum connections per server                 min_connections       usize       2       Minimum idle connections per server           Multiple Memcached Servers   let config = MemcachedConfig {     servers: vec![         \"memcached-01:11211\".to_string(),         \"memcached-02:11211\".to_string(),         \"memcached-03:11211\".to_string(),     ],     max_connections: 20,     min_connections: 5, };  let backend = MemcachedBackend::new(config)?;   Key distribution: Keys are automatically distributed across servers using consistent hashing.   Memcached Best Practices   ✅ DO:     Use multiple servers for redundancy   Set appropriate TTLs (no persistence)   Monitor memory usage per server   Plan for cache misses (no persistence)   ❌ DON’T:     Rely on wildcard delete operations (not supported)   Expect data to survive restarts   Use for long-term storage   Ignore server failures (no automatic failover)     Tier-1: Development &amp; Testing   InMemory Backend   The InMemory backend uses an in-process concurrent HashMap (DashMap).   Why Choose InMemory?      ✅ Zero dependencies — No external services needed   ✅ Fast setup — Perfect for local development   ✅ Deterministic — Same process, predictable behavior   ✅ Thread-safe — Lock-free concurrent access   ⚠️ Limitations:     ❌ Single instance — Not distributed   ❌ Memory-only — Data lost on process restart   ❌ Not scalable — Limited to single machine   Installation   InMemory backend is included by default:   [dependencies] cache-kit = \"0.9\"   Configuration   use cache_kit::backend::InMemoryBackend; use cache_kit::CacheExpander;  let backend = InMemoryBackend::new(); let expander = CacheExpander::new(backend);   No configuration needed! Perfect for:     Unit tests   Integration tests   Local development   Proof-of-concept projects   InMemory Best Practices   ✅ DO:     Use for all unit tests   Use for local development   Create fresh instances per test   Clear cache between tests if needed   ❌ DON’T:     Use in production   Share instances across tests (isolation)   Expect data to survive process restarts   Use for distributed services     Backend Comparison                  Feature       Redis       Memcached       InMemory                       Performance       ⚡⚡ Very Fast       ⚡⚡⚡ Ultra Fast       ⚡⚡⚡ Ultra Fast                 Persistence       ✅ Optional       ❌ No       ❌ No                 Distribution       ✅ Clustering       ✅ Multi-server       ❌ Single process                 Complexity       Medium       Low       Very Low                 Setup Time       Minutes       Minutes       Seconds                 Production Ready       ✅ Yes       ✅ Yes       ❌ No                 Data Structures       ✅ Rich       ❌ Key-Value only       ❌ Key-Value only                 Memory Management       ✅ Eviction policies       ✅ LRU       ⚠️ Manual                 Pub/Sub       ✅ Yes       ❌ No       ❌ No                 Transactions       ✅ Yes       ❌ No       ❌ No             Choosing the Right Backend   Decision Tree   Are you in production? ├─ Yes → Need persistence? │   ├─ Yes → Redis │   └─ No → Need extreme speed? │       ├─ Yes → Memcached │       └─ No → Redis └─ No → Local development / testing?     └─ Yes → InMemory   Use Case Recommendations                  Use Case       Recommended Backend       Rationale                       Production web app       Redis       Persistence, rich features, managed services                 High-traffic API       Memcached       Ultra-fast, distributed                 Session storage       Redis       Persistence, expiry, pub/sub                 Read-heavy workload       Memcached       Optimized for reads                 Local development       InMemory       Zero setup, fast iterations                 Unit tests       InMemory       Deterministic, isolated                 Multi-region deployment       Redis       Replication, clustering             Switching Backends   Switching backends requires no code changes in your application logic:   // Development (InMemory) #[cfg(debug_assertions)] let backend = InMemoryBackend::new();  // Production (Redis) #[cfg(not(debug_assertions))] let backend = RedisBackend::new(RedisConfig {     host: std::env::var(\"REDIS_HOST\").unwrap_or_else(|_| \"localhost\".to_string()),     port: std::env::var(\"REDIS_PORT\")         .ok()         .and_then(|p| p.parse().ok())         .unwrap_or(6379),     ..Default::default() })?;  // Same expander interface let expander = CacheExpander::new(backend);   Or use environment variables:   fn create_backend() -&gt; Box&lt;dyn cache_kit::backend::CacheBackend&gt; {     match std::env::var(\"CACHE_BACKEND\").as_deref() {         Ok(\"redis\") =&gt; {             let host = std::env::var(\"REDIS_HOST\")                 .unwrap_or_else(|_| \"localhost\".to_string());             let port = std::env::var(\"REDIS_PORT\")                 .ok()                 .and_then(|p| p.parse().ok())                 .unwrap_or(6379);             Box::new(RedisBackend::new(RedisConfig {                 host,                 port,                 ..Default::default()             }).expect(\"Failed to connect to Redis\"))         }         Ok(\"memcached\") =&gt; {             let servers = std::env::var(\"MEMCACHED_SERVERS\")                 .expect(\"MEMCACHED_SERVERS required\")                 .split(',')                 .map(String::from)                 .collect();             Box::new(MemcachedBackend::new(MemcachedConfig {                 servers,                 ..Default::default()             }).expect(\"Failed to connect to Memcached\"))         }         _ =&gt; Box::new(InMemoryBackend::new()),     } }     Connection Pooling   Redis and Memcached backends use connection pooling for optimal performance.   Pool Configuration   use std::time::Duration;  let config = RedisConfig {     host: \"localhost\".to_string(),     port: 6379,     pool_size: 16,    // Optimized default (8 cores × 2 + 1 ≈ 16)     connection_timeout: Duration::from_secs(10),     username: None,     password: None,     database: 0, };   Sizing Guidelines   Recommended formula: (CPU cores × 2) + 1                  System       Formula       Recommended Pool Size                       4-core system       (4 × 2) + 1 = 9       8-10                 8-core system       (8 × 2) + 1 = 17       16 (default)                 16-core system       (16 × 2) + 1 = 33       32                 32-core system       (32 × 2) + 1 = 65       64           Research findings: Pool size of 16 provides 49-53% latency reduction compared to pool size of 10 on 8-core systems, with 2.75x reduction in contention outliers (22% → 8%). See Performance Guide for detailed benchmarks.   Default: cache-kit uses max_connections: 16 and min_connections: 4 as optimized defaults for typical 8-core systems.     Docker Compose Setup   For local development:   version: '3.8'  services:   redis:     image: redis:7-alpine     ports:       - \"6379:6379\"     volumes:       - redis_data:/data     command: redis-server --appendonly yes    memcached:     image: memcached:1.6-alpine     ports:       - \"11211:11211\"     command: memcached -m 64  # 64MB memory limit  volumes:   redis_data:   Start services:   docker-compose up -d   Test connectivity:   # Redis redis-cli ping  # Should return: PONG  # Memcached echo \"stats\" | nc localhost 11211     Monitoring and Observability   Redis Monitoring   # Connection count redis-cli CLIENT LIST | wc -l  # Memory usage redis-cli INFO memory | grep used_memory_human  # Hit rate redis-cli INFO stats | grep keyspace   Memcached Monitoring   # Stats echo \"stats\" | nc localhost 11211  # Hit rate echo \"stats\" | nc localhost 11211 | grep -E \"cmd_get|get_hits|get_misses\"   Application Metrics   Implement cache metrics in your application:   struct CacheMetrics {     hits: AtomicU64,     misses: AtomicU64,     errors: AtomicU64, }  impl CacheMetrics {     fn hit_rate(&amp;self) -&gt; f64 {         let hits = self.hits.load(Ordering::Relaxed);         let misses = self.misses.load(Ordering::Relaxed);         if hits + misses == 0 {             return 0.0;         }         hits as f64 / (hits + misses) as f64     } }     Production Checklist   Before deploying to production:   Redis      Enable persistence (AOF or RDB)   Set maxmemory policy   Enable authentication (requirepass)   Use TLS for network traffic   Configure backup strategy   Monitor memory usage   Set up replication or clustering   Memcached      Deploy multiple servers   Configure memory limits   Monitor server health   Plan for cache misses (no persistence)   Set up monitoring alerts   InMemory      ⚠️ Not recommended for production   Use only for proof-of-concept or single-instance services     Next Steps      Review Design principles   Explore the Actix + SQLx reference implementation   Read about Serialization formats   Check the Installation guide  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/backends/",
        "teaser": null
      },{
        "title": "Core Concepts",
        "excerpt":"  Overview   cache-kit is built around four core concepts that work together to provide clean, explicit caching boundaries:      Serializable Entities — Type-safe data models   Deterministic Cache Keys — Consistent, predictable addressing   Explicit Cache Boundaries — Clear ownership and behavior   Cache Invalidation Control — You decide when data becomes stale   These concepts are intentionally simple and avoid framework-specific abstractions.     Serializable Entities   An entity in cache-kit is any Rust type that can be:      Serialized to bytes (for storage in cache)   Deserialized from bytes (for retrieval from cache)   Cloned (for internal cache operations)   Identified by a unique key   The CacheEntity Trait   use cache_kit::CacheEntity; use serde::{Deserialize, Serialize};  #[derive(Clone, Serialize, Deserialize)] struct User {     id: String,     name: String,     email: String, }  impl CacheEntity for User {     type Key = String;      fn cache_key(&amp;self) -&gt; Self::Key {         self.id.clone()     }      fn cache_prefix() -&gt; &amp;'static str {         \"user\"     } }   What Makes an Entity Cacheable?                  Requirement       Purpose                       Clone       Cache operations need to duplicate entities                 Serialize       Convert to bytes for storage                 Deserialize       Convert from bytes for retrieval                 Send + Sync       Safe to share across threads                 cache_key()       Unique identifier for this entity                 cache_prefix()       Namespace for entity type           Cache Key Construction   The final cache key is constructed as:   {prefix}:{key}   For the User example above:   let user = User {     id: \"user_001\".to_string(),     name: \"Alice\".to_string(),     email: \"alice@example.com\".to_string(), };  // Final cache key: \"user:user_001\"   This pattern ensures:     No collisions between different entity types   Predictable keys for debugging and monitoring   Type safety at compile time     Deterministic Cache Keys   Cache keys must be deterministic — given the same entity, you always get the same key.   Good Key Examples   // ✅ Simple ID impl CacheEntity for User {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key {         self.id.clone()     } }  // ✅ Composite key impl CacheEntity for OrderItem {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key {         format!(\"{}:{}\", self.order_id, self.item_id)     } }  // ✅ Numeric ID impl CacheEntity for Product {     type Key = u64;     fn cache_key(&amp;self) -&gt; Self::Key {         self.product_id     } }   Anti-Patterns to Avoid   // ❌ Non-deterministic (timestamp) fn cache_key(&amp;self) -&gt; String {     format!(\"{}:{}\", self.id, SystemTime::now().timestamp()) }  // ❌ Non-deterministic (random) fn cache_key(&amp;self) -&gt; String {     format!(\"{}:{}\", self.id, rand::random::&lt;u64&gt;()) }  // ❌ Overly complex (hash collisions possible) fn cache_key(&amp;self) -&gt; String {     format!(\"{:x}\", calculate_hash(&amp;self)) }   Rule: Cache keys should depend only on stable entity attributes.     Explicit Cache Boundaries   cache-kit uses a feeder pattern to define explicit cache boundaries.   The CacheFeed Trait   A feeder acts as a bridge between cache-kit and your application:   use cache_kit::CacheFeed;  struct UserFeeder {     id: String,     user: Option&lt;User&gt;, }  impl CacheFeed&lt;User&gt; for UserFeeder {     fn entity_id(&amp;mut self) -&gt; String {         self.id.clone()     }      fn feed(&amp;mut self, entity: Option&lt;User&gt;) {         self.user = entity;     } }   Why Feeders?   Feeders provide several benefits:      Explicit data flow — You control where cached data goes   Type safety — Compiler enforces correct usage   No hidden state — No implicit global caches   Testability — Easy to mock and verify   Feeder Lifecycle   1. Create feeder with entity ID         ↓ 2. Pass feeder to cache expander         ↓ 3. Cache expander calls entity_id()         ↓ 4. Cache hit → feed() called with entity    Cache miss → fetch from repository → feed() called         ↓ 5. Application reads entity from feeder   Example: Using a Feeder   // 1. Create feeder with the ID you want to fetch let mut feeder = UserFeeder {     id: \"user_001\".to_string(),     user: None, };  // 2. Execute cache operation expander.with(&amp;mut feeder, &amp;repository, CacheStrategy::Refresh)?;  // 3. Access the result if let Some(user) = feeder.user {     println!(\"Found user: {}\", user.name); } else {     println!(\"User not found\"); }     Cache Strategies   cache-kit provides four explicit cache strategies:   1. Fresh (Cache-Only)   CacheStrategy::Fresh      Behavior: Return entity from cache, or None if not cached   Use case: When you ONLY want cached data, never database   Example: Real-time dashboards showing last known state   cache.execute(&amp;mut feeder, &amp;repository, CacheStrategy::Fresh).await?;  match feeder.user {     Some(user) =&gt; println!(\"Cached user: {}\", user.name),     None =&gt; println!(\"Not in cache\"), }   2. Refresh (Cache + Database Fallback)   CacheStrategy::Refresh      Behavior: Try cache first, fallback to database on miss, then cache the result   Use case: Default and recommended for most operations   Example: User profile lookups, product details   cache.execute(&amp;mut feeder, &amp;repository, CacheStrategy::Refresh).await?;  // Will always have data (if it exists in DB) if let Some(user) = feeder.user {     println!(\"User: {}\", user.name); }   3. Invalidate (Clear + Refresh)   CacheStrategy::Invalidate      Behavior: Remove from cache, fetch from database, cache the fresh result   Use case: After updates/writes to ensure fresh data   Example: After user updates profile   // User updated their profile repository.update_user(&amp;updated_user)?;  // Invalidate cache and fetch fresh data expander.with(&amp;mut feeder, &amp;repository, CacheStrategy::Invalidate)?;   4. Bypass (Database-Only)   CacheStrategy::Bypass      Behavior: Skip cache entirely, always fetch from database   Use case: One-off queries, debugging, auditing   Example: Admin operations that need guaranteed fresh data   // Always fetch from database, ignore cache cache.execute(&amp;mut feeder, &amp;repository, CacheStrategy::Bypass).await?;   Strategy Decision Tree   Need data?   ├─ Only cached? → Fresh   ├─ Fresh from DB required? → Invalidate or Bypass   ├─ Normal read? → Refresh (default)   └─ Debugging? → Bypass     Data Repository Pattern   cache-kit is agnostic to your data source. You define how to fetch entities:   The DataRepository Trait   use cache_kit::DataRepository;  pub trait DataRepository&lt;T: CacheEntity&gt;: Send + Sync {     async fn fetch_by_id(&amp;self, id: &amp;T::Key) -&gt; cache_kit::Result&lt;Option&lt;T&gt;&gt;; }   Example: SQLx Repository   use sqlx::PgPool;  struct UserRepository {     pool: PgPool, }  impl DataRepository&lt;User&gt; for UserRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         let user = sqlx::query_as!(             User,             \"SELECT id, name, email FROM users WHERE id = $1\",             id         )         .fetch_optional(&amp;self.pool)         .await         .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;          Ok(user)     } }   Example: In-Memory Repository (for Testing)   use std::collections::HashMap; use std::sync::{Arc, Mutex};  struct InMemoryRepository {     data: Arc&lt;Mutex&lt;HashMap&lt;String, User&gt;&gt;&gt;, }  impl DataRepository&lt;User&gt; for InMemoryRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         let data = self.data.lock().unwrap();         Ok(data.get(id).cloned())     } }   Repository Best Practices   ✅ DO:     Keep repositories focused on data fetching only   Return Option&lt;T&gt; to distinguish “not found” from errors   Use proper error types (convert DB errors to cache-kit errors)   Make repositories cloneable (Arc wrapper)   ❌ DON’T:     Put cache logic inside repositories   Mix business logic with data access   Assume entities exist (always return Option)   Panic on database errors     Cache Ownership and Invalidation   You own cache invalidation. cache-kit does not:      Automatically invalidate on writes   Track entity relationships   Provide distributed invalidation   Guess when data is stale   Invalidation Patterns   Pattern 1: Invalidate After Write   use cache_kit::{CacheService, backend::InMemoryBackend, CacheStrategy};  pub struct UserService {     cache: CacheService&lt;InMemoryBackend&gt;,     repository: UserRepository, }  impl UserService {     pub async fn update_user(&amp;self, user: &amp;User) -&gt; cache_kit::Result&lt;()&gt; {         // 1. Update database         self.repository.update(user).await?;          // 2. Invalidate cache         let mut feeder = UserFeeder {             id: user.id.clone(),             user: None,         };         self.cache.execute(             &amp;mut feeder,             &amp;self.repository,             CacheStrategy::Invalidate         ).await?;          Ok(())     } }   Pattern 2: TTL-Based Expiry   use std::time::Duration; use cache_kit::{CacheService, backend::InMemoryBackend};  // Create cache (TTL managed by backend configuration) let cache = CacheService::new(InMemoryBackend::new());  // Data expiry is configured at backend level cache.execute(&amp;mut feeder, &amp;repository, CacheStrategy::Refresh).await?;   Pattern 3: Event-Driven Invalidation   use cache_kit::{CacheService, backend::InMemoryBackend, CacheStrategy};  // When user updates profile via event async fn on_user_updated(     cache: &amp;CacheService&lt;InMemoryBackend&gt;,     repository: &amp;UserRepository,     event: UserUpdatedEvent ) -&gt; cache_kit::Result&lt;()&gt; {     let mut feeder = UserFeeder {         id: event.user_id,         user: None,     };      // Clear cache for this user     cache.execute(&amp;mut feeder, repository, CacheStrategy::Invalidate).await?;     Ok(()) }     Putting It All Together   Here’s how all concepts work together:   use cache_kit::{     CacheEntity, CacheFeed, DataRepository, CacheService,     backend::InMemoryBackend,     strategy::CacheStrategy, }; use serde::{Deserialize, Serialize};  // 1. Entity (Serializable) #[derive(Clone, Serialize, Deserialize)] struct Product {     id: u64,     name: String,     price: f64, }  // 2. Deterministic cache key impl CacheEntity for Product {     type Key = u64;     fn cache_key(&amp;self) -&gt; Self::Key { self.id }     fn cache_prefix() -&gt; &amp;'static str { \"product\" } }  // 3. Explicit cache boundary (Feeder) struct ProductFeeder {     id: u64,     product: Option&lt;Product&gt;, }  impl CacheFeed&lt;Product&gt; for ProductFeeder {     fn entity_id(&amp;mut self) -&gt; u64 { self.id }     fn feed(&amp;mut self, entity: Option&lt;Product&gt;) { self.product = entity; } }  // 4. Data repository struct ProductRepository;  impl DataRepository&lt;Product&gt; for ProductRepository {     async fn fetch_by_id(&amp;self, id: &amp;u64) -&gt; cache_kit::Result&lt;Option&lt;Product&gt;&gt; {         // Your database logic         Ok(Some(Product {             id: *id,             name: \"Example Product\".to_string(),             price: 99.99,         }))     } }  // Usage #[tokio::main] async fn main() -&gt; cache_kit::Result&lt;()&gt; {     let cache = CacheService::new(InMemoryBackend::new());     let repository = ProductRepository;      let mut feeder = ProductFeeder {         id: 123,         product: None,     };      // Cache operation with explicit strategy     cache.execute(&amp;mut feeder, &amp;repository, CacheStrategy::Refresh).await?;      if let Some(product) = feeder.product {         println!(\"Product: {} - ${}\", product.name, product.price);     }      Ok(()) }     Next Steps      Install and configure cache-kit in your project   Learn about Database &amp; ORM compatibility   Explore Serialization options   Review Cache backend choices  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/concepts/",
        "teaser": null
      },{
        "title": "Database & ORM Compatibility",
        "excerpt":"  ORM-Agnostic Design   cache-kit does not depend on ORMs.   It operates on three simple concepts:     Serializable entities — Any type implementing CacheEntity   Deterministic cache keys — Consistent identifiers   Explicit cache boundaries — Clear separation via CacheFeed   This means:     ✅ Swap ORMs without changing cache logic   ✅ Use multiple ORMs in the same application   ✅ Cache data from any source (DB, API, file system)     Supported ORMs &amp; Database Layers   Tier-1: Recommended (with Examples)                  ORM       Status       Example       Notes                       SQLx       ✅ Full Support       actixsqlx       Async-first, compile-time checked SQL           Tier-1: Compatible (Community Examples Welcome)                  ORM       Status       Example       Notes                       SeaORM       ✅ Compatible       Community contributions welcome       Async ORM with migrations                 Diesel       ✅ Compatible       Community contributions welcome       Mature, type-safe ORM                 tokio-postgres       ✅ Compatible       Works with any database layer       Pure async PostgreSQL client           Tier-2: Any Database Layer   cache-kit works with any Rust code that can:     Fetch entities by ID   Return Option&lt;T&gt; (entity or not found)   Implement DataRepository&lt;T&gt; trait   This includes:     Custom SQL builders   NoSQL databases (MongoDB, DynamoDB)   REST API clients   File-based storage   In-memory data structures     Conceptual Flow   ┌─────────────────────┐ │ Database / ORM      │ ← Your choice └──────────┬──────────┘            │            ↓ Fetch entities ┌─────────────────────┐ │ Domain Entities     │ ← impl CacheEntity └──────────┬──────────┘            │            ↓ Cache operations ┌─────────────────────┐ │ cache-kit           │ ← Framework-agnostic └──────────┬──────────┘            │            ↓ Store/retrieve ┌─────────────────────┐ │ Cache Backend       │ ← Redis, Memcached, InMemory └─────────────────────┘   Key principle: Database models live in your database layer, cache-kit just coordinates caching.     SQLx Integration   SQLx is an async, compile-time checked SQL library. It’s the recommended database layer for new projects.   Installation   [dependencies] cache-kit = \"0.9\" sqlx = { version = \"0.8\", features = [\"runtime-tokio\", \"postgres\", \"uuid\", \"chrono\"] } tokio = { version = \"1.41\", features = [\"full\"] } serde = { version = \"0.9\", features = [\"derive\"] }   Entity Definition   use serde::{Deserialize, Serialize}; use cache_kit::CacheEntity;  #[derive(Clone, Serialize, Deserialize, sqlx::FromRow)] pub struct User {     pub id: String,     pub username: String,     pub email: String,     pub created_at: chrono::DateTime&lt;chrono::Utc&gt;, }  impl CacheEntity for User {     type Key = String;      fn cache_key(&amp;self) -&gt; Self::Key {         self.id.clone()     }      fn cache_prefix() -&gt; &amp;'static str {         \"user\"     } }   Repository Implementation   use cache_kit::DataRepository; use sqlx::PgPool;  pub struct UserRepository {     pool: PgPool, }  impl UserRepository {     pub fn new(pool: PgPool) -&gt; Self {         Self { pool }     }      pub async fn create(&amp;self, user: &amp;User) -&gt; Result&lt;User, sqlx::Error&gt; {         sqlx::query_as!(             User,             r#\"             INSERT INTO users (id, username, email)             VALUES ($1, $2, $3)             RETURNING *             \"#,             user.id,             user.username,             user.email         )         .fetch_one(&amp;self.pool)         .await     }      pub async fn update(&amp;self, user: &amp;User) -&gt; Result&lt;User, sqlx::Error&gt; {         sqlx::query_as!(             User,             r#\"             UPDATE users             SET username = $2, email = $3             WHERE id = $1             RETURNING *             \"#,             user.id,             user.username,             user.email         )         .fetch_one(&amp;self.pool)         .await     }      pub async fn delete(&amp;self, id: &amp;str) -&gt; Result&lt;(), sqlx::Error&gt; {         sqlx::query!(\"DELETE FROM users WHERE id = $1\", id)             .execute(&amp;self.pool)             .await?;         Ok(())     } }  impl DataRepository&lt;User&gt; for UserRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         let user = sqlx::query_as!(             User,             \"SELECT * FROM users WHERE id = $1\",             id         )         .fetch_optional(&amp;self.pool)         .await         .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;          Ok(user)     } }   Usage in Service Layer   use cache_kit::{CacheService, CacheFeed, DataRepository, strategy::CacheStrategy}; use cache_kit::backend::InMemoryBackend; use std::sync::Arc;  pub struct UserService {     cache: CacheService&lt;InMemoryBackend&gt;,     repo: Arc&lt;UserRepository&gt;, }  impl UserService {     pub async fn get_user(&amp;self, id: &amp;str) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         let mut feeder = UserFeeder {             id: id.to_string(),             user: None,         };          self.cache             .execute(&amp;mut feeder, &amp;*self.repo, CacheStrategy::Refresh)             .await?;          Ok(feeder.user)     } }   See the full Actix + SQLx example for a complete implementation.     SeaORM Integration  use std::sync::Arc; use cache_kit::{CacheExpander, strategy::CacheStrategy}; use cache_kit::backend::InMemoryBackend;   pub struct UserService {     repo: Arc,     cache: Arc&lt;CacheExpander&gt;, }   impl UserService {     pub fn new(repo: Arc, cache: Arc&lt;CacheExpander&gt;) -&gt; Self {         Self { repo, cache }     }   pub async fn get(&amp;self, id: &amp;str) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {     let mut feeder = UserFeeder {         id: id.to_string(),         user: None,     };      self.cache.with(&amp;mut feeder, &amp;*self.repo, CacheStrategy::Refresh)?;      Ok(feeder.user) }  pub async fn create(&amp;self, user: User) -&gt; cache_kit::Result&lt;User&gt; {     // 1. Insert into database     let created = self.repo.create(&amp;user).await         .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;      // 2. Cache the new entity     let mut feeder = UserFeeder {         id: created.id.clone(),         user: Some(created.clone()),     };     self.cache.with(&amp;mut feeder, &amp;*self.repo, CacheStrategy::Refresh)?;      Ok(created) }  pub async fn update(&amp;self, user: User) -&gt; cache_kit::Result&lt;User&gt; {     // 1. Update database     let updated = self.repo.update(&amp;user).await         .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;      // 2. Invalidate cache and fetch fresh     let mut feeder = UserFeeder {         id: updated.id.clone(),         user: None,     };     self.cache.with(&amp;mut feeder, &amp;*self.repo, CacheStrategy::Invalidate)?;      Ok(updated) }  pub async fn delete(&amp;self, id: &amp;str) -&gt; cache_kit::Result&lt;()&gt; {     // 1. Delete from database     self.repo.delete(id).await         .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;      // 2. Invalidate cache     let mut feeder = UserFeeder {         id: id.to_string(),         user: None,     };     self.cache.with(&amp;mut feeder, &amp;*self.repo, CacheStrategy::Invalidate)?;      Ok(()) } } ```   See the complete actixsqlx example for a full working implementation.     SeaORM Integration   SeaORM is an async ORM with migrations and schema management.   Entity Definition   use sea_orm::entity::prelude::*; use serde::{Deserialize, Serialize}; use cache_kit::CacheEntity;  #[derive(Clone, Debug, PartialEq, DeriveEntityModel, Serialize, Deserialize)] #[sea_orm(table_name = \"users\")] pub struct Model {     #[sea_orm(primary_key)]     pub id: String,     pub username: String,     pub email: String, }  #[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)] pub enum Relation {}  impl ActiveModelBehavior for ActiveModel {}  // Implement CacheEntity for the SeaORM Model impl CacheEntity for Model {     type Key = String;      fn cache_key(&amp;self) -&gt; Self::Key {         self.id.clone()     }      fn cache_prefix() -&gt; &amp;'static str {         \"user\"     } }   Repository Implementation   use sea_orm::{DatabaseConnection, EntityTrait}; use cache_kit::DataRepository;  pub struct UserRepository {     db: DatabaseConnection, }  impl DataRepository&lt;user::Model&gt; for UserRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;user::Model&gt;&gt; {         let user = user::Entity::find_by_id(id.clone())             .one(&amp;self.db)             .await             .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;          Ok(user)     } }   Community contributions welcome! Share your SeaORM + cache-kit examples.     Diesel Integration   Diesel is a mature, type-safe ORM with excellent compile-time guarantees.   Entity Definition   use diesel::prelude::*; use serde::{Deserialize, Serialize}; use cache_kit::CacheEntity;  #[derive(Queryable, Selectable, Clone, Serialize, Deserialize)] #[diesel(table_name = users)] pub struct User {     pub id: String,     pub username: String,     pub email: String, }  impl CacheEntity for User {     type Key = String;      fn cache_key(&amp;self) -&gt; Self::Key {         self.id.clone()     }      fn cache_prefix() -&gt; &amp;'static str {         \"user\"     } }   Repository Implementation (Sync)   use diesel::prelude::*; use diesel::r2d2::{ConnectionManager, Pool}; use cache_kit::DataRepository;  pub struct UserRepository {     pool: Pool&lt;ConnectionManager&lt;PgConnection&gt;&gt;, }  impl DataRepository&lt;User&gt; for UserRepository {     fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         use crate::schema::users::dsl::*;          let mut conn = self.pool.get()             .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;          let user = users             .filter(id.eq(id))             .first::&lt;User&gt;(&amp;mut conn)             .optional()             .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;          Ok(user)     } }   Note: Diesel is synchronous, which works perfectly with cache-kit’s sync DataRepository trait.   Community contributions welcome! Share your Diesel + cache-kit examples.     Custom Database Layer   cache-kit works with any custom database layer:   use cache_kit::DataRepository;  pub struct CustomRepository {     // Your custom database client     client: MyDatabaseClient, }  impl DataRepository&lt;MyEntity&gt; for CustomRepository {     fn fetch_by_id(&amp;self, id: &amp;MyEntityId) -&gt; cache_kit::Result&lt;Option&lt;MyEntity&gt;&gt; {         // Your custom fetching logic         match self.client.query(id) {             Ok(Some(data)) =&gt; Ok(Some(MyEntity::from(data))),             Ok(None) =&gt; Ok(None),             Err(e) =&gt; Err(cache_kit::Error::RepositoryError(e.to_string())),         }     } }     Database Best Practices   Separate Concerns   ✅ Good:     Database Models → Repository → Cache → Service → API  ❌ Bad:     Database Models with embedded cache logic   Repository Pattern   Keep repositories focused on data access:   impl UserRepository {     // ✅ Simple, focused data access     pub async fn find_by_id(&amp;self, id: &amp;str) -&gt; Result&lt;Option&lt;User&gt;, DbError&gt; {         sqlx::query_as!(...).await     }      // ❌ Don't mix cache logic in repository     pub async fn find_by_id_cached(&amp;self, id: &amp;str) -&gt; Result&lt;Option&lt;User&gt;, DbError&gt; {         // BAD: Repository shouldn't know about caching     } }   Error Handling   Convert database errors to cache-kit errors:   impl DataRepository&lt;User&gt; for UserRepository {     fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         self.internal_fetch(id)             .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))     } }     Database Migrations   cache-kit does not handle database migrations. Use your ORM’s migration tools:   SQLx   sqlx migrate add create_users_table   Edit migrations/001_create_users_table.sql:   CREATE TABLE users (     id VARCHAR(255) PRIMARY KEY,     username VARCHAR(100) NOT NULL UNIQUE,     email VARCHAR(255) NOT NULL UNIQUE,     created_at TIMESTAMPTZ NOT NULL DEFAULT NOW() );  CREATE INDEX idx_users_username ON users(username); CREATE INDEX idx_users_email ON users(email);   Run migrations:   sqlx migrate run --database-url postgres://localhost/mydb   SeaORM   sea-orm-cli migrate generate create_users_table   Diesel   diesel migration generate create_users_table     Next Steps      Learn about API Frameworks and Transport Layers   Explore Serialization options   Review Cache backend choices   See the Actix + SQLx reference implementation  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/database-compatibility/",
        "teaser": null
      },{
        "title": "Positioning & Design Principles",
        "excerpt":"  Core Philosophy   cache-kit is designed around three fundamental principles:      Boundaries, not ownership   Explicit behavior, not hidden magic   Integration, not lock-in   These principles guide every design decision in cache-kit.     Boundaries, Not Ownership   cache-kit does not try to own your application stack.   What This Means   cache-kit integrates around your existing choices:   ┌─────────────────────────────────────────┐ │           Your Choices                  │ │  • Framework (Axum, Actix, Tonic)       │ │  • ORM (SQLx, SeaORM, Diesel)           │ │  • Transport (HTTP, gRPC, Workers)      │ │  • Runtime (tokio)                      │ └──────────────┬──────────────────────────┘                │                ↓ Cache operations ┌─────────────────────────────────────────┐ │          cache-kit                      │ │  Places clear boundaries               │ │  Does NOT dictate architecture         │ └─────────────────────────────────────────┘   Design Decisions                  What cache-kit Does       What cache-kit Does NOT Do                       ✅ Provide cache operations       ❌ Replace your ORM                 ✅ Define cache boundaries       ❌ Manage HTTP routing                 ✅ Handle serialization       ❌ Impose web frameworks                 ✅ Support multiple backends       ❌ Require specific databases                 ✅ Integrate with async       ❌ Create runtimes           Benefits      Freedom of choice — Use any framework, ORM, transport   Evolutionary architecture — Swap components independently   Library-safe — Use inside SDKs and libraries   No vendor lock-in — cache-kit is just one piece     Explicit Behavior, Not Hidden Magic   cache-kit makes cache behavior visible and predictable.   No Implicit Caching   // ❌ WRONG: Hidden caching (magic) fn get_user(id: &amp;str) -&gt; User {     // Automatically cached somewhere?     // How? When? For how long?     database.query(id) }  // ✅ RIGHT: Explicit caching (cache-kit) fn get_user(id: &amp;str) -&gt; Result&lt;Option&lt;User&gt;&gt; {     let mut feeder = UserFeeder { id: id.to_string(), user: None };      // Explicit: I know this uses cache     // Explicit: I chose the strategy     // Explicit: I control the result     cache.with(&amp;mut feeder, &amp;repository, CacheStrategy::Refresh)?;      Ok(feeder.user) }   Explicit Invalidation   cache-kit does NOT:     Automatically invalidate on writes   Guess when data is stale   Track entity relationships   Provide “magic” cache eviction   You decide when to invalidate:   impl UserService {     pub async fn update_user(&amp;self, user: User) -&gt; Result&lt;User&gt; {         // 1. Update database         let updated = self.repo.update(&amp;user).await?;          // 2. Explicitly invalidate cache         let mut feeder = UserFeeder {             id: updated.id.clone(),             user: None,         };         self.cache.with(&amp;mut feeder, &amp;self.repo, CacheStrategy::Invalidate)?;          Ok(updated)     } }   Explicit Strategies   Four cache strategies, each with clear semantics:                  Strategy       Behavior       Use When                       Fresh       Cache-only       You ONLY want cached data                 Refresh       Cache + DB fallback       Normal reads (default)                 Invalidate       Clear + refresh       After writes                 Bypass       Skip cache       Debugging, auditing           No guessing. No surprises.     Integration, Not Lock-In   cache-kit is designed to play well with others.   Framework Agnostic   Works with any framework:   // Axum async fn axum_handler(State(cache): State&lt;Arc&lt;CacheExpander&lt;_&gt;&gt;&gt;) -&gt; Result&lt;Json&lt;User&gt;&gt; {     // Same cache operations }  // Actix async fn actix_handler(data: web::Data&lt;AppState&gt;) -&gt; HttpResponse {     // Same cache operations }  // Tonic (gRPC) async fn grpc_method(&amp;self, request: Request&lt;UserRequest&gt;) -&gt; Result&lt;Response&lt;UserResponse&gt;&gt; {     // Same cache operations }   The same cache logic works across all frameworks.   ORM Agnostic   Works with any database layer:   // SQLx impl DataRepository&lt;User&gt; for SqlxRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; Result&lt;Option&lt;User&gt;&gt; {         let user = sqlx::query_as!(             User,             \"SELECT * FROM users WHERE id = $1\",             id         )         .fetch_optional(&amp;self.pool)         .await         .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;          Ok(user)     } }  // SeaORM impl DataRepository&lt;User&gt; for SeaOrmRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; Result&lt;Option&lt;User&gt;&gt; {         let user = Entity::find_by_id(id)             .one(&amp;self.db)             .await             .map_err(|e| cache_kit::Error::RepositoryError(e.to_string()))?;          Ok(user)     } }   Backend Agnostic   Swap backends with zero code changes:   // Development let backend = InMemoryBackend::new();  // Production let backend = RedisBackend::new(config)?;  // Same interface let expander = CacheExpander::new(backend);     Guarantees and Non-Guarantees   cache-kit is explicit about what it guarantees and what it does not.   What cache-kit Guarantees   ✅ Type safety — Compiler-verified cache operations ✅ Thread safety — Send + Sync everywhere ✅ Deterministic keys — Same entity → same key ✅ No silent failures — All errors are propagated ✅ Backend abstraction — Swap backends without code changes ✅ Async-first — Built for tokio-based apps   What cache-kit Does NOT Guarantee   ❌ Strong consistency — Distributed caches are eventually consistent ❌ Automatic invalidation — You control when data is invalidated ❌ Distributed coordination — No locks, no consensus ❌ Eviction policies — Depends on backend (Redis, Memcached) ❌ Persistence — Depends on backend (Redis has persistence, Memcached doesn’t) ❌ Cross-language compatibility — Postcard is Rust-only     Design Patterns   Service Layer Pattern (Recommended)   HTTP Handler → Service Layer → Cache → Repository → Database   Benefits:     Clean separation of concerns   Reusable across transports   Testable in isolation   Example:   pub struct UserService {     cache: Arc&lt;CacheExpander&lt;RedisBackend&gt;&gt;,     repo: Arc&lt;UserRepository&gt;, }  impl UserService {     // Business logic + caching     pub fn get_user(&amp;self, id: &amp;str) -&gt; Result&lt;Option&lt;User&gt;&gt; {         let mut feeder = UserFeeder { id: id.to_string(), user: None };         self.cache.with(&amp;mut feeder, &amp;*self.repo, CacheStrategy::Refresh)?;         Ok(feeder.user)     } }  // Use in HTTP handler async fn handler(service: Arc&lt;UserService&gt;) -&gt; Result&lt;Json&lt;User&gt;&gt; {     service.get_user(\"user_001\")  // Clean and simple }  // Use in gRPC handler async fn grpc_handler(service: Arc&lt;UserService&gt;) -&gt; Result&lt;Response&lt;UserResponse&gt;&gt; {     service.get_user(\"user_001\")  // Same logic! }   Repository Pattern   // Repository: Only data access impl DataRepository&lt;User&gt; for UserRepository {     fn fetch_by_id(&amp;self, id: &amp;String) -&gt; Result&lt;Option&lt;User&gt;&gt; {         // Pure database logic     } }  // Service: Business logic + caching impl UserService {     pub fn get_user(&amp;self, id: &amp;str) -&gt; Result&lt;Option&lt;User&gt;&gt; {         // Cache coordination     }      pub async fn update_user(&amp;self, user: User) -&gt; Result&lt;User&gt; {         // Write + cache invalidation     } }   Feeder Pattern   // Feeder: Explicit data flow struct UserFeeder {     id: String,     user: Option&lt;User&gt;, }  impl CacheFeed&lt;User&gt; for UserFeeder {     fn entity_id(&amp;mut self) -&gt; String { self.id.clone() }     fn feed(&amp;mut self, entity: Option&lt;User&gt;) { self.user = entity; } }  // Usage: Clear and traceable let mut feeder = UserFeeder { id: \"user_001\".to_string(), user: None }; cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)?; println!(\"Result: {:?}\", feeder.user);  // Explicit data flow     Trade-Offs and Honesty   cache-kit makes intentional trade-offs and is honest about them.   Trade-Off 1: Postcard vs JSON                  Aspect       Postcard (Chosen)       JSON (Alternative)                       Performance       ⚡ 10-15x faster       ❌ Baseline                 Size       📦 40-50% smaller       ❌ Baseline                 Decimal support       ❌ No       ✅ Yes                 Language support       ❌ Rust-only       ✅ Many languages           Decision: Prioritize performance for Rust-to-Rust caching. Decimal limitation is documented and workarounds are provided.   Trade-Off 2: Async DataRepository                  Aspect       Async (Chosen)                       Native async support       ✅ Direct .await                 Modern Rust practices       ✅ Idiomatic async/await                 Compatibility       ✅ SQLx, SeaORM, tokio-postgres                 Ecosystem alignment       ✅ Works with modern async frameworks           Decision: Use async trait for modern async databases. This is the recommended pattern for Rust services.   Trade-Off 3: Explicit Invalidation vs Automatic                  Aspect       Explicit (Chosen)       Automatic (Alternative)                       Control       ✅ Full control       ❌ Hidden behavior                 Predictability       ✅ Predictable       ⚠️ Can surprise you                 Complexity       ✅ Simple       ❌ Complex dependency tracking           Decision: Make invalidation explicit. No magic, no surprises.     Safety and Reliability   Thread Safety   All cache-kit types are Send + Sync:   // Safe to share across threads let cache = Arc::new(CacheExpander::new(backend));  // Safe to use in async tasks tokio::spawn(async move {     let mut feeder = UserFeeder { ... };     cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)?; });   Error Handling   cache-kit never panics in normal operation:   // All operations return Result match cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh) {     Ok(_) =&gt; println!(\"Success\"),     Err(e) =&gt; eprintln!(\"Cache error: {}\", e), }   Memory Safety      No unsafe code in cache-kit core   All backends use safe Rust   DashMap (InMemory) is lock-free and safe     Library and SDK Use   cache-kit is safe to use inside libraries:   // Inside a library crate pub struct MyLibrary {     cache: CacheExpander&lt;InMemoryBackend&gt;,     // or bring-your-own-backend pattern }  impl MyLibrary {     pub fn new() -&gt; Self {         Self {             cache: CacheExpander::new(InMemoryBackend::new()),         }     }      // Your library methods     pub fn fetch_data(&amp;mut self, id: &amp;str) -&gt; Result&lt;Data&gt; {         let mut feeder = DataFeeder { ... };         self.cache.with(&amp;mut feeder, &amp;self.repo, CacheStrategy::Refresh)?;         // ...     } }   Benefits:     No framework dependencies   No global state   No runtime assumptions   Safe to embed     When NOT to Use cache-kit   cache-kit is not the right choice if you need:   ❌ Distributed locks — Use a coordination service (etcd, ZooKeeper) ❌ Strong consistency — Use a distributed database (Spanner, CockroachDB) ❌ Cross-language caching — Use JSON or MessagePack (when available) ❌ Automatic schema migration — cache-kit uses explicit versioning ❌ All-in-one framework — cache-kit is just a caching library     Design Goals Summary                  Goal       How cache-kit Achieves It                       Simple       Four cache strategies, clear semantics                 Fast       Postcard serialization, async-first                 Type-safe       Compile-time verified operations                 Flexible       Works with any ORM, framework, backend                 Honest       Explicit about trade-offs and limitations                 Predictable       No magic, explicit behavior                 Safe       Send + Sync, no panics, safe Rust                 Integrable       Fits into existing architectures             Contributing to cache-kit   cache-kit follows these principles in all contributions:   Code Contributions   ✅ Preferred:     Backend implementations (MessagePack, new cache backends)   ORM examples (SeaORM, Diesel)   Documentation improvements   Bug fixes with tests   ⚠️ Discouraged:     Breaking API changes without strong justification   Framework-specific features   Magic or implicit behavior   Features that increase complexity significantly   Documentation Contributions   ✅ Encouraged:     Real-world examples   Integration guides   Performance comparisons   Best practice documentation   See CONTRIBUTING.md for details.     References and Inspiration   cache-kit is inspired by:      SeaORM — Clean, composable Rust ORM   Exonum — Type-safe service boundaries   Rust ecosystem — async-first, zero-cost abstractions     Next Steps      Start with Installation   Review Core Concepts   Explore the Actix + SQLx reference implementation   Join the community on GitHub  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/design-principles/",
        "teaser": null
      },{
        "title": "Contributing Guide",
        "excerpt":"Thank you for contributing! This guide covers code standards, testing requirements, and submission process.     Quick Links      Development Guides: See ARCHITECTURE_*.md files for detailed implementation guides   Code Examples: See examples/ directory   Testing Guide: See TESTING.md     Table of Contents      Getting Started   Development Workflow   Architecture Documentation   Code Style &amp; Standards   Testing Requirements   Submitting Changes     Getting Started   Prerequisites      Rust 1.75 or higher   Cargo   Git   Clone and Build   git clone https://github.com/megamsys/cache-kit.rs cd cache-kit.rs  # Build cargo build --all-features  # Run tests cargo test --all-features  # Run examples cargo run --example basic_usage     Development Workflow   1. Choose What to Work On      Check GitHub Issues   Review ARCHITECTURE_*.md files for planned features   Propose new features by opening an issue first   2. Create a Branch   git checkout -b feature/my-feature # or git checkout -b fix/issue-123   3. Make Changes   Follow the Code Style &amp; Standards below.   4. Test Your Changes   See Testing Requirements below.   5. Submit Pull Request   See Submitting Changes below.     Architecture Documentation   For detailed implementation guides, see the architecture documents:   Implementation Guides      ARCHITECTURE_01_ACTIX_EXAMPLE.md - Actix Web framework integration            REST API endpoints with caching       Complete code examples       Testing strategies           ARCHITECTURE_02_SERIALIZATION.md - Efficient serialization formats            Bincode and MessagePack support       CacheSerializer trait implementation       Performance benchmarks           ARCHITECTURE_03_BENCHMARK.md - Performance benchmarking            Criterion setup and configuration       Benchmark groups and test cases       Performance baselines           ARCHITECTURE_04_REGISTRY.md - Transactional cache registry            Multi-entity cache operations       Transaction support with rollback       Relationship graph for cascade invalidation           When Building Extensions   Before implementing a custom backend, feeder, metrics, or repository:      Review the relevant ARCHITECTURE_*.md file   Look at existing implementations in src/   Check examples/ for usage patterns   Follow the trait definitions in the architecture docs   Project Structure   cache-kit/ ├── src/ │   ├── lib.rs                    # Library entry point │   ├── entity.rs                 # CacheEntity&lt;T&gt; trait │   ├── feed.rs                   # CacheFeed&lt;T&gt; trait │   ├── repository.rs             # DataRepository&lt;T&gt; trait │   ├── strategy.rs               # CacheStrategy enum │   ├── expander.rs               # CacheExpander main orchestrator │   ├── key.rs                    # Key management utilities │   ├── observability.rs          # Metrics &amp; TTL traits │   ├── error.rs                  # Error types │   └── backend/ │       ├── mod.rs                # CacheBackend trait │       ├── inmemory.rs           # InMemoryBackend (reference) │       ├── redis.rs              # RedisBackend │       └── memcached.rs          # MemcachedBackend ├── examples/                     # Usage examples ├── tests/                        # Integration tests ├── ARCHITECTURE_*.md             # Implementation guides └── README.md                     # Project overview     Code Style &amp; Standards   Formatting   All code must be formatted with rustfmt:   cargo fmt   Linting   All code must pass clippy without warnings:   cargo clippy --all-targets --all-features -- -D warnings   Documentation      Write doc comments (///) for all public items   Include examples in doc comments for important functions   Keep line length ≤ 100 characters   Use //! for module-level documentation   Example:   /// Fetches a cached entity or falls back to the repository. /// /// # Arguments /// /// * `feeder` - The feeder that will receive the entity /// * `repository` - The repository to fetch from on cache miss /// * `strategy` - The cache strategy to use /// /// # Returns /// /// * `Ok(())` - Operation succeeded /// * `Err(Error)` - Operation failed /// /// # Example /// /// ``` /// use cache_kit::*; /// use cache_kit::backend::InMemoryBackend; /// use cache_kit::strategy::CacheStrategy; /// /// let mut expander = CacheExpander::new(InMemoryBackend::new()); /// expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)?; /// # Ok::&lt;(), Box&lt;dyn std::error::Error&gt;&gt;(()) /// ``` pub fn with&lt;T, F&gt;(...) { }   Error Handling      Use Result&lt;T, Error&gt; for all fallible operations   Convert external errors to crate::error::Error   Provide meaningful error messages   Don’t use unwrap() or expect() in library code   Thread Safety      All public types must be Send + Sync where appropriate   Use Arc and Mutex/RwLock for shared state   Document any thread safety assumptions     Testing Requirements   Unit Tests   Every module should have unit tests:   #[cfg(test)] mod tests {     use super::*;      #[test]     fn test_basic_functionality() {         // Test implementation     } }   Integration Tests   Add integration tests in tests/ directory for end-to-end scenarios.   Running Tests   # Run all tests cargo test --all-features  # Run tests for specific feature cargo test --features redis  # Run with logging RUST_LOG=debug cargo test  # Run specific test cargo test test_name   Coverage Requirements      New features should have &gt;80% test coverage   Bug fixes should include regression tests   Breaking changes require updated tests   Test Checklist      Unit tests for new code   Integration tests for new features   All tests pass: cargo test --all-features   No test warnings   Tests are deterministic (no flaky tests)     Submitting Changes   Pre-Submission Checklist   Before submitting a pull request, ensure:   # 1. Code builds cargo build --all-features  # 2. Tests pass cargo test --all-features  # 3. Clippy passes cargo clippy --all-targets --all-features -- -D warnings  # 4. Code is formatted cargo fmt --check  # 5. Documentation builds cargo doc --no-deps   Commit Message Format   Write clear, descriptive commit messages:   Short description (50 chars max)  Longer explanation if needed. Wrap at 72 characters.  - Bullet points are fine - Use them to list key changes  Fixes #123   Examples:      Add Redis transaction support for atomic operations   Fix cache key generation for composite keys   Update documentation for builder pattern   Pull Request Process      Fork the repository   Create a feature branch: git checkout -b feature/my-feature   Make changes following guidelines above   Add tests for your changes   Run pre-submission checklist (see above)   Commit with clear messages   Push to your fork: git push origin feature/my-feature   Create Pull Request with:            Clear title and description       Link to related issues       Description of changes       Testing performed           Pull Request Review      Maintainers will review your PR   Address feedback and comments   Update your PR as needed   Once approved, it will be merged     Development Tips   Testing Backends Locally   Redis:   # Start Redis with Docker docker run -d -p 6379:6379 redis:latest  # Run Redis tests cargo test --features redis   Memcached:   # Start Memcached with Docker docker run -d -p 11211:11211 memcached:latest  # Run Memcached tests cargo test --features memcached   Debugging   Enable debug logging:   RUST_LOG=debug cargo test test_name -- --nocapture   Benchmarking   See the Performance Guide for comprehensive benchmarking with Criterion, including baseline comparison and regression detection.     Version Management   Releasing a New Version   Version Management:   cache-kit uses a VERSION file as the single source of truth.   To bump version:   # Updates VERSION, Cargo.toml, README.md, and all docs/*.md files make version-bump VERSION=0.10.0   Pre-Release Checklist:      Run: make version-bump VERSION=0.10.0   Update CHANGELOG.md with release notes   Run tests: cargo test --all-features   Commit: git commit -m \"Release v0.10.0\"   Tag: git tag v0.10.0   Push: git push origin main --tags   Publish: cargo publish   Note: build.rs validates VERSION matches Cargo.toml at compile time.   Code Example Best Practices   Use semantic versioning in documentation examples:   # ✅ Recommended: Semantic versioning cache-kit = \"1\"                    # Latest 1.x.x cache-kit = { version = \"1\", features = [\"redis\"] }  # ❌ Avoid: Exact versions (requires update for every patch) cache-kit = \"0.9.0\"   This ensures code examples don’t need updating for minor/patch releases.     Questions?      Issues: GitHub Issues   Discussions: GitHub Discussions   Email: nkishore@megam.io     License   By contributing, you agree that your contributions will be licensed under the MIT License.  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/guides/contributing/",
        "teaser": null
      },{
        "title": "Examples & Patterns",
        "excerpt":"This document covers advanced cache framework patterns and usage scenarios.     Table of Contents      Composite Keys   Builder Pattern   Registry Pattern   TTL Strategies   Error Handling   Batch Operations   Multi-Tier Caching   Custom Serialization   Async Patterns     Composite Keys   Use composite keys for entities that depend on multiple parameters.   Example: Employment by Person and Date   use cache_kit::CacheEntity; use serde::{Deserialize, Serialize}; use std::fmt::Display;  #[derive(Clone, Serialize, Deserialize, Debug)] pub struct Employment {     pub person_id: String,     pub year: i32,     pub employer: String,     pub salary: f64, }  // Define a composite key type #[derive(Clone, Eq, PartialEq, Hash, Debug)] pub struct EmploymentKey {     person_id: String,     year: i32, }  impl Display for EmploymentKey {     fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result {         write!(f, \"{}:{}\", self.person_id, self.year)     } }  impl CacheEntity for Employment {     type Key = EmploymentKey;      fn cache_key(&amp;self) -&gt; Self::Key {         EmploymentKey {             person_id: self.person_id.clone(),             year: self.year,         }     }      fn cache_prefix() -&gt; &amp;'static str {         \"employment\"     } }  // Usage struct EmploymentFeeder {     person_id: String,     year: i32,     pub employment: Option&lt;Employment&gt;, }  impl CacheFeed&lt;Employment&gt; for EmploymentFeeder {     fn entity_id(&amp;mut self) -&gt; String {         // Both person_id and year become the entity_id         format!(\"{}:{}\", self.person_id, self.year)     }      fn feed(&amp;mut self, entity: Option&lt;Employment&gt;) {         self.employment = entity;     } }  // Fetch employment for a specific person and year fn fetch_employment(     expander: &amp;mut CacheExpander&lt;InMemoryBackend&gt;,     person_id: String,     year: i32, ) {     let mut feeder = EmploymentFeeder {         person_id,         year,         employment: None,     };      let repository = EmploymentRepository;     expander.with(&amp;mut feeder, &amp;repository, CacheStrategy::Refresh).ok();      if let Some(emp) = feeder.employment {         println!(\"Employment: {:?}\", emp);     } }     Builder Pattern   Use a builder for complex cache configurations.   use cache_kit::{CacheExpander, observability::TtlPolicy}; use std::time::Duration;  pub struct CacheBuilder&lt;B: CacheBackend&gt; {     expander: CacheExpander&lt;B&gt;,     ttl_policy: Option&lt;TtlPolicy&gt;,     metrics: Option&lt;Box&lt;dyn CacheMetrics&gt;&gt;, }  impl&lt;B: CacheBackend&gt; CacheBuilder&lt;B&gt; {     pub fn new(backend: B) -&gt; Self {         CacheBuilder {             expander: CacheExpander::new(backend),             ttl_policy: None,             metrics: None,         }     }      pub fn with_ttl(mut self, policy: TtlPolicy) -&gt; Self {         self.ttl_policy = Some(policy);         self     }      pub fn with_metrics(mut self, metrics: Box&lt;dyn CacheMetrics&gt;) -&gt; Self {         self.metrics = Some(metrics);         self     }      pub fn build(mut self) -&gt; CacheExpander&lt;B&gt; {         if let Some(metrics) = self.metrics {             self.expander = self.expander.with_metrics(metrics);         }         if let Some(ttl) = self.ttl_policy {             self.expander = self.expander.with_ttl_policy(ttl);         }         self.expander     } }  // Usage fn setup_cache() -&gt; CacheExpander&lt;InMemoryBackend&gt; {     let backend = InMemoryBackend::new();          let ttl_policy = TtlPolicy::PerType(Box::new(|entity_type| {         match entity_type {             \"employment\" =&gt; Duration::from_secs(3600),             \"borrower\" =&gt; Duration::from_secs(7200),             _ =&gt; Duration::from_secs(1800),         }     }));      CacheBuilder::new(backend)         .with_ttl(ttl_policy)         .with_metrics(Box::new(MyMetrics))         .build() }     Registry Pattern   Use a registry for managing multiple feeders and repositories.   use std::collections::HashMap; use cache_kit::{CacheEntity, CacheFeed, DataRepository, CacheExpander};  pub struct CacheRegistry&lt;B: CacheBackend&gt; {     expander: CacheExpander&lt;B&gt;,     feeders: HashMap&lt;String, Box&lt;dyn std::any::Any&gt;&gt;, }  impl&lt;B: CacheBackend&gt; CacheRegistry&lt;B&gt; {     pub fn new(backend: B) -&gt; Self {         CacheRegistry {             expander: CacheExpander::new(backend),             feeders: HashMap::new(),         }     }      /// Register a feeder by name     pub fn register_feeder&lt;T: 'static&gt;(         &amp;mut self,         name: String,         feeder: Box&lt;dyn std::any::Any&gt;,     ) {         self.feeders.insert(name, feeder);     }      /// Get a registered feeder     pub fn get_feeder&lt;T: 'static&gt;(&amp;self, name: &amp;str) -&gt; Option&lt;&amp;T&gt; {         self.feeders             .get(name)             .and_then(|f| f.downcast_ref::&lt;T&gt;())     } }  // Simpler approach: Direct registry for specific types pub struct EntityRegistry {     employment_feeders: Vec&lt;(String, Option&lt;Employment&gt;)&gt;,     borrower_feeders: Vec&lt;(String, Option&lt;Borrower&gt;)&gt;, }  impl EntityRegistry {     pub fn new() -&gt; Self {         EntityRegistry {             employment_feeders: Vec::new(),             borrower_feeders: Vec::new(),         }     }      pub fn fetch_employment_batch(         &amp;mut self,         expander: &amp;mut CacheExpander&lt;InMemoryBackend&gt;,         ids: Vec&lt;String&gt;,         repository: &amp;dyn DataRepository&lt;Employment&gt;,     ) {         for id in ids {             let mut feeder = GenericFeeder::&lt;Employment&gt;::new(id.clone());             expander.with(&amp;mut feeder, repository, CacheStrategy::Refresh).ok();             self.employment_feeders.push((id, feeder.entity));         }     }      pub fn fetch_borrower_batch(         &amp;mut self,         expander: &amp;mut CacheExpander&lt;InMemoryBackend&gt;,         ids: Vec&lt;String&gt;,         repository: &amp;dyn DataRepository&lt;Borrower&gt;,     ) {         for id in ids {             let mut feeder = GenericFeeder::&lt;Borrower&gt;::new(id.clone());             expander.with(&amp;mut feeder, repository, CacheStrategy::Refresh).ok();             self.borrower_feeders.push((id, feeder.entity));         }     } }  // Usage fn batch_fetch_entities() {     let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);     let mut registry = EntityRegistry::new();      let employment_repo = EmploymentRepository;     let ids = vec![\"emp_1\".to_string(), \"emp_2\".to_string(), \"emp_3\".to_string()];      registry.fetch_employment_batch(&amp;mut expander, ids, &amp;employment_repo);      for (id, employment) in registry.employment_feeders.iter() {         if let Some(emp) = employment {             println!(\"Employment {}: {:?}\", id, emp);         }     } }     TTL Strategies   Different TTL approaches for different scenarios.   Fixed TTL (Simple)   use cache_kit::observability::TtlPolicy; use std::time::Duration;  let ttl = TtlPolicy::Fixed(Duration::from_secs(3600)); // 1 hour   Per-Type TTL   let ttl = TtlPolicy::PerType(Box::new(|entity_type| {     match entity_type {         // Short-lived volatile data         \"transaction\" =&gt; Duration::from_secs(60),                  // Medium-lived data         \"employment\" =&gt; Duration::from_secs(1800),         \"borrower\" =&gt; Duration::from_secs(3600),                  // Long-lived stable data         \"lending_term\" =&gt; Duration::from_secs(86400),         \"site_settings\" =&gt; Duration::from_secs(86400),                  // Default         _ =&gt; Duration::from_secs(1800),     } }));   Context-Aware TTL   use cache_kit::observability::TtlPolicy; use cache_kit::strategy::CacheContext; use std::time::Duration;  let ttl = TtlPolicy::Custom(Box::new(|context| {     // Different TTL based on operation     match context.strategy {         // Fresh data is cached longer         CacheStrategy::Fresh =&gt; Some(Duration::from_secs(7200)),                  // Refresh data for balance (1 hour)         CacheStrategy::Refresh =&gt; Some(Duration::from_secs(3600)),                  // Invalidate has very short TTL         CacheStrategy::Invalidate =&gt; Some(Duration::from_secs(300)),                  // Bypass doesn't cache         CacheStrategy::Bypass =&gt; None,                  _ =&gt; Some(Duration::from_secs(1800)),     } }));   User Role-Based TTL   let ttl = TtlPolicy::PerType(Box::new(|user_role| {     match user_role {         // Admin sees stale data for longer (trusted)         \"admin\" =&gt; Duration::from_secs(7200),                  // Regular user gets fresher data         \"user\" =&gt; Duration::from_secs(1800),                  // Guest gets very fresh data         \"guest\" =&gt; Duration::from_secs(300),                  _ =&gt; Duration::from_secs(1800),     } }));     Error Handling   Proper error handling patterns.   Graceful Degradation   use cache_kit::error::Error;  fn fetch_with_fallback(     expander: &amp;mut CacheExpander&lt;InMemoryBackend&gt;,     feeder: &amp;mut impl CacheFeed&lt;Employment&gt;,     repository: &amp;dyn DataRepository&lt;Employment&gt;, ) {     match expander.with(feeder, repository, CacheStrategy::Refresh) {         Ok(_) =&gt; {             println!(\"Successfully fetched from cache or database\");         }         Err(Error::SerializationError(e)) =&gt; {             eprintln!(\"Serialization error (corrupted cache?): {}\", e);             // Try to fetch fresh copy             let _ = expander.with(feeder, repository, CacheStrategy::Invalidate);         }         Err(Error::RepositoryError(e)) =&gt; {             eprintln!(\"Database error: {}\", e);             // Try cache-only             let _ = expander.with(feeder, repository, CacheStrategy::Fresh);         }         Err(Error::BackendError(e)) =&gt; {             eprintln!(\"Cache backend error: {}\", e);             // Bypass cache, go directly to DB             let _ = expander.with(feeder, repository, CacheStrategy::Bypass);         }         Err(e) =&gt; {             eprintln!(\"Other error: {:?}\", e);         }     } }   Retry Logic   use std::time::Duration;  fn fetch_with_retry(     expander: &amp;mut CacheExpander&lt;InMemoryBackend&gt;,     feeder: &amp;mut impl CacheFeed&lt;Employment&gt;,     repository: &amp;dyn DataRepository&lt;Employment&gt;,     max_retries: u32, ) -&gt; Result&lt;()&gt; {     let mut retries = 0;          loop {         match expander.with(feeder, repository, CacheStrategy::Refresh) {             Ok(_) =&gt; return Ok(()),             Err(e) =&gt; {                 retries += 1;                 if retries &gt;= max_retries {                     return Err(e);                 }                                  eprintln!(\"Retry {}/{}: {:?}\", retries, max_retries, e);                 std::thread::sleep(Duration::from_millis(100 * retries as u64));             }         }     } }     Batch Operations   Cache multiple entities efficiently.   Batch Get   pub fn batch_get&lt;T: CacheEntity&gt;(     expander: &amp;mut CacheExpander&lt;impl CacheBackend&gt;,     ids: Vec&lt;String&gt;,     repository: &amp;dyn DataRepository&lt;T&gt;, ) -&gt; Vec&lt;Option&lt;T&gt;&gt; {     let mut results = Vec::new();          for id in ids {         let mut feeder = GenericFeeder::&lt;T&gt;::new(id);                  if expander.with(&amp;mut feeder, repository, CacheStrategy::Refresh).is_ok() {             results.push(feeder.entity);         } else {             results.push(None);         }     }          results }  // Usage let employment_ids = vec![\"emp_1\".to_string(), \"emp_2\".to_string(), \"emp_3\".to_string()]; let results = batch_get::&lt;Employment&gt;(&amp;mut expander, employment_ids, &amp;repository);   Parallel Batch Processing   use std::sync::Arc; use std::sync::Mutex;  pub fn batch_get_parallel&lt;T: CacheEntity + Send + 'static&gt;(     backend: Arc&lt;InMemoryBackend&gt;,     ids: Vec&lt;String&gt;,     repository: Arc&lt;dyn DataRepository&lt;T&gt;&gt;, ) -&gt; Vec&lt;Option&lt;T&gt;&gt; {     let results = Arc::new(Mutex::new(Vec::new()));     let mut handles = vec![];      for id in ids {         let backend_clone = Arc::clone(&amp;backend);         let repo_clone = Arc::clone(&amp;repository);         let results_clone = Arc::clone(&amp;results);          let handle = std::thread::spawn(move || {             let mut expander = CacheExpander::new((*backend_clone).clone());             let mut feeder = GenericFeeder::&lt;T&gt;::new(id);              if let Ok(_) = expander.with(&amp;mut feeder, &amp;*repo_clone, CacheStrategy::Refresh) {                 results_clone.lock().unwrap().push(feeder.entity);             }         });          handles.push(handle);     }      for handle in handles {         handle.join().unwrap();     }      Arc::into_inner(results)         .unwrap()         .into_inner()         .unwrap() }     Multi-Tier Caching   Implement L1 (in-memory) and L2 (Redis) cache tiers.   use cache_kit::CacheExpander; use cache_kit::backend::{InMemoryBackend, RedisBackend};  pub struct TieredCache {     l1: CacheExpander&lt;InMemoryBackend&gt;, // Fast, local     l2: CacheExpander&lt;RedisBackend&gt;,    // Distributed }  impl TieredCache {     pub fn new(l1_backend: InMemoryBackend, l2_backend: RedisBackend) -&gt; Self {         TieredCache {             l1: CacheExpander::new(l1_backend),             l2: CacheExpander::new(l2_backend),         }     }      pub fn fetch&lt;T, F&gt;(         &amp;mut self,         feeder: &amp;mut F,         repository: &amp;dyn DataRepository&lt;T&gt;,         strategy: CacheStrategy,     ) -&gt; Result&lt;()&gt;     where         T: CacheEntity,         F: CacheFeed&lt;T&gt;,     {         // Try L1 first (fastest)         match self.l1.with(feeder, repository, CacheStrategy::Fresh) {             Ok(_) =&gt; {                 println!(\"L1 cache hit\");                 return Ok(());             }             Err(_) =&gt; {                 // L1 miss, try L2                 println!(\"L1 cache miss, trying L2\");             }         }          // Try L2 (distributed)         match self.l2.with(feeder, repository, CacheStrategy::Fresh) {             Ok(_) =&gt; {                 println!(\"L2 cache hit, populating L1\");                 // Populate L1 for next access                 self.l1.with(feeder, repository, CacheStrategy::Refresh).ok();                 return Ok(());             }             Err(_) =&gt; {                 // L2 miss, fetch from DB                 println!(\"L2 cache miss, fetching from database\");             }         }          // Fetch from database and populate both tiers         self.l2.with(feeder, repository, strategy.clone())?;         self.l1.with(feeder, repository, strategy)?;         Ok(())     } }  // Usage fn main() -&gt; Result&lt;()&gt; {     let l1 = InMemoryBackend::new();     let l2 = RedisBackend::new(RedisConfig::default())?;          let mut cache = TieredCache::new(l1, l2);          let mut feeder = GenericFeeder::&lt;Employment&gt;::new(\"emp_1\".to_string());     let repo = EmploymentRepository;          cache.fetch(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)?;          Ok(()) }     Custom Serialization   Override default serialization for specific entities.   use cache_kit::CacheEntity; use serde_json::json;  #[derive(Clone, Serialize, Deserialize)] pub struct LargeEntity {     pub id: String,     pub data: Vec&lt;u8&gt;, // Large binary data }  impl CacheEntity for LargeEntity {     type Key = String;      fn cache_key(&amp;self) -&gt; Self::Key {         self.id.clone()     }      fn cache_prefix() -&gt; &amp;'static str {         \"large\"     }      /// Use MessagePack instead of JSON for efficiency     fn serialize_for_cache(&amp;self) -&gt; Result&lt;Vec&lt;u8&gt;&gt; {         rmp_serde::to_vec(self)             .map_err(|e| Error::SerializationError(e.to_string()))     }      fn deserialize_from_cache(bytes: &amp;[u8]) -&gt; Result&lt;Self&gt; {         rmp_serde::from_slice(bytes)             .map_err(|e| Error::SerializationError(e.to_string()))     } }     Async Patterns   cache-kit is async-first. Use direct .await calls:   use cache_kit::{CacheExpander, backend::CacheBackend, CacheEntity, CacheFeed, DataRepository, CacheStrategy};  pub async fn fetch_cached&lt;T, F, R&gt;(     expander: &amp;CacheExpander&lt;impl CacheBackend&gt;,     feeder: &amp;mut F,     repository: &amp;R,     strategy: CacheStrategy, ) -&gt; cache_kit::Result&lt;()&gt; where     T: CacheEntity,     F: CacheFeed&lt;T&gt;,     R: DataRepository&lt;T&gt;, {     // Direct async call - no blocking needed     expander.with::&lt;T, F, R&gt;(feeder, repository, strategy).await }  // Usage in async context #[tokio::main] async fn main() -&gt; cache_kit::Result&lt;()&gt; {     let backend = InMemoryBackend::new();     let expander = CacheExpander::new(backend);      let mut feeder = GenericFeeder::&lt;Employment&gt;::new(\"emp_1\".to_string());     let repo = EmploymentRepository;      fetch_cached(&amp;expander, &amp;mut feeder, &amp;repo, CacheStrategy::Refresh).await?;      Ok(()) }   Async Batch Processing   use cache_kit::{CacheService, CacheEntity, CacheFeed, DataRepository, strategy::CacheStrategy}; use cache_kit::backend::InMemoryBackend; use std::sync::Arc;  pub async fn fetch_batch_async&lt;T: CacheEntity + Send + 'static&gt;(     cache: CacheService&lt;InMemoryBackend&gt;,     ids: Vec&lt;String&gt;,     repository: Arc&lt;dyn DataRepository&lt;T&gt; + Send + Sync&gt;, ) -&gt; Vec&lt;Option&lt;T&gt;&gt; {     let mut tasks = vec![];      for id in ids {         let cache_clone = cache.clone();         let repo_clone = Arc::clone(&amp;repository);          let task = tokio::spawn(async move {             let mut feeder = GenericFeeder::&lt;T&gt;::new(id);              if cache_clone.execute(&amp;mut feeder, &amp;*repo_clone, CacheStrategy::Refresh).await.is_ok() {                 feeder.entity             } else {                 None             }         });          tasks.push(task);     }      let mut results = vec![];     for task in tasks {         if let Ok(result) = task.await {             results.push(result);         }     }      results }     See Also      CONTRIBUTING.md - How to extend the framework   TESTING.md - Testing strategies   README.md - Quick start guide  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/guides/examples/",
        "teaser": null
      },{
        "title": "Guides",
        "excerpt":"Practical guides for using and contributing to cache-kit.   Available Guides      Examples &amp; Patterns - Advanced usage patterns and real-world scenarios   Testing Guide - Testing strategies, Makefile commands, and CI/CD integration   Performance Guide - Benchmarking, optimization, and production tuning   Production Troubleshooting - Diagnose and resolve common cache-kit issues with error handling best practices   Monitoring &amp; Metrics - Set up observability, metrics, and alerting   Contributing Guide - Code standards, testing requirements, and submission process   Quick Start   If you’re new to cache-kit, start with the Contributing Guide to understand the project structure and development workflow.   For advanced usage patterns and real-world examples, see the Examples &amp; Patterns guide.   To run tests and validate your changes, refer to the Testing Guide.  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/guides/index/",
        "teaser": null
      },{
        "title": "Monitoring & Metrics",
        "excerpt":"Monitoring &amp; Metrics Guide   Set up production-grade monitoring, metrics, and observability for cache-kit.     Overview   Monitoring cache-kit allows you to:     Detect problems early — Catch issues before users notice   Understand performance — Hit rates, latency, throughput   Optimize configuration — Data-driven tuning decisions   Alert on degradation — Automated on-call notifications   Troubleshoot quickly — Historical data for diagnosis   The 4 Golden Signals for Caching      Latency — How fast are cache operations? (p50, p99)   Traffic — How much are we using the cache? (ops/sec)   Errors — What percentage of operations fail? (error rate)   Hit rate — What percentage of requests hit cache? (cache efficiency)     Metrics Implementation   Simple Metrics Struct   use std::sync::atomic::{AtomicU64, Ordering}; use std::sync::Arc; use std::time::Instant;  #[derive(Clone)] pub struct CacheMetrics {     hits: Arc&lt;AtomicU64&gt;,     misses: Arc&lt;AtomicU64&gt;,     errors: Arc&lt;AtomicU64&gt;,     latency_total_us: Arc&lt;AtomicU64&gt;,     latency_count: Arc&lt;AtomicU64&gt;, }  impl CacheMetrics {     pub fn new() -&gt; Self {         CacheMetrics {             hits: Arc::new(AtomicU64::new(0)),             misses: Arc::new(AtomicU64::new(0)),             errors: Arc::new(AtomicU64::new(0)),             latency_total_us: Arc::new(AtomicU64::new(0)),             latency_count: Arc::new(AtomicU64::new(0)),         }     }      /// Record a cache hit     pub fn record_hit(&amp;self, latency_us: u64) {         self.hits.fetch_add(1, Ordering::Relaxed);         self.latency_total_us.fetch_add(latency_us, Ordering::Relaxed);         self.latency_count.fetch_add(1, Ordering::Relaxed);     }      /// Record a cache miss     pub fn record_miss(&amp;self, latency_us: u64) {         self.misses.fetch_add(1, Ordering::Relaxed);         self.latency_total_us.fetch_add(latency_us, Ordering::Relaxed);         self.latency_count.fetch_add(1, Ordering::Relaxed);     }      /// Record a cache error     pub fn record_error(&amp;self, latency_us: u64) {         self.errors.fetch_add(1, Ordering::Relaxed);         self.latency_total_us.fetch_add(latency_us, Ordering::Relaxed);         self.latency_count.fetch_add(1, Ordering::Relaxed);     }      /// Get current hit rate (0.0 to 1.0)     pub fn hit_rate(&amp;self) -&gt; f64 {         let hits = self.hits.load(Ordering::Relaxed) as f64;         let misses = self.misses.load(Ordering::Relaxed) as f64;         if (hits + misses) == 0.0 {             return 0.0;         }         hits / (hits + misses)     }      /// Get error rate (0.0 to 1.0)     pub fn error_rate(&amp;self) -&gt; f64 {         let errors = self.errors.load(Ordering::Relaxed) as f64;         let total = self.hits.load(Ordering::Relaxed) as f64                   + self.misses.load(Ordering::Relaxed) as f64                   + errors;         if total == 0.0 {             return 0.0;         }         errors / total     }      /// Get average latency in microseconds     pub fn avg_latency_us(&amp;self) -&gt; f64 {         let count = self.latency_count.load(Ordering::Relaxed);         if count == 0 {             return 0.0;         }         let total = self.latency_total_us.load(Ordering::Relaxed) as f64;         total / count as f64     }      /// Get total operations     pub fn total_ops(&amp;self) -&gt; u64 {         self.hits.load(Ordering::Relaxed)             + self.misses.load(Ordering::Relaxed)             + self.errors.load(Ordering::Relaxed)     }      /// Get throughput (ops/sec) — pass elapsed_secs     pub fn throughput_ops_sec(&amp;self, elapsed_secs: f64) -&gt; f64 {         self.total_ops() as f64 / elapsed_secs     } }   Instrument Your Code   pub async fn get_user_with_metrics(     cache: &amp;mut CacheExpander&lt;impl CacheBackend&gt;,     repo: &amp;UserRepository,     user_id: String,     metrics: &amp;CacheMetrics, ) -&gt; Result&lt;Option&lt;User&gt;&gt; {     let start = Instant::now();     let mut feeder = UserFeeder {         id: user_id.clone(),         user: None,     };      match cache.with(&amp;mut feeder, repo, CacheStrategy::Refresh) {         Ok(_) =&gt; {             let latency_us = start.elapsed().as_micros() as u64;             if feeder.user.is_some() {                 metrics.record_hit(latency_us);                 info!(\"Cache HIT for user {}\", user_id);             } else {                 metrics.record_miss(latency_us);                 info!(\"Cache MISS for user {}\", user_id);             }             Ok(feeder.user)         }         Err(e) =&gt; {             let latency_us = start.elapsed().as_micros() as u64;             metrics.record_error(latency_us);             error!(\"Cache ERROR for user {}: {}\", user_id, e);             Err(e)         }     } }     Prometheus Integration   Expose Metrics Endpoint   See the Axum example for a complete working implementation with:     Metrics HTTP endpoint   API server with cache instrumentation   Prometheus scrape configuration   The metrics endpoint exposes the standard Prometheus format:  cache_hits_total (counter) cache_misses_total (counter) cache_errors_total (counter) cache_hit_rate (gauge, 0.0-1.0) cache_error_rate (gauge, 0.0-1.0) cache_avg_latency_us (gauge)   Prometheus Scrape Configuration   # prometheus.yml global:   scrape_interval: 15s   evaluation_interval: 15s  scrape_configs:   - job_name: 'cache-kit'     static_configs:       - targets: ['localhost:3000']     metrics_path: '/metrics'   Alert Rules   # alerts.yml groups:    - name: cache-kit      interval: 30s      rules:        # Alert if hit rate drops below 30%        - alert: LowCacheHitRate          expr: cache_hit_rate &lt; 0.3          for: 5m          labels:            severity: warning          annotations:            summary: \"Low cache hit rate ({{ $value | humanizePercentage }})\"            description: \"Cache hit rate below 30% for 5 minutes\"         # Alert if error rate exceeds 5%        - alert: HighCacheErrorRate          expr: cache_error_rate &gt; 0.05          for: 2m          labels:            severity: critical          annotations:            summary: \"High cache error rate ({{ $value | humanizePercentage }})\"            description: \"Cache errors above 5% - likely backend down\"         # Alert if average latency exceeds 100ms        - alert: SlowCacheLatency          expr: cache_avg_latency_us &gt; 0.9.00          for: 5m          labels:            severity: warning          annotations:            summary: \"Slow cache latency ({{ $value | humanizeDuration }})\"            description: \"Cache operations averaging &gt; 100ms\"     Grafana Dashboard   Dashboard JSON   {   \"dashboard\": {     \"title\": \"cache-kit Metrics\",     \"panels\": [       {         \"title\": \"Cache Hit Rate\",         \"targets\": [           {             \"expr\": \"cache_hit_rate\"           }         ],         \"fieldConfig\": {           \"defaults\": {             \"unit\": \"percentunit\",             \"thresholds\": {               \"steps\": [                 { \"color\": \"red\", \"value\": 0 },                 { \"color\": \"yellow\", \"value\": 0.3 },                 { \"color\": \"green\", \"value\": 0.7 }               ]             }           }         }       },       {         \"title\": \"Operations Per Second\",         \"targets\": [           {             \"expr\": \"rate(cache_hits_total[1m]) + rate(cache_misses_total[1m])\"           }         ]       },       {         \"title\": \"Error Rate\",         \"targets\": [           {             \"expr\": \"cache_error_rate\"           }         ],         \"fieldConfig\": {           \"defaults\": {             \"unit\": \"percentunit\",             \"thresholds\": {               \"steps\": [                 { \"color\": \"green\", \"value\": 0 },                 { \"color\": \"red\", \"value\": 0.01 }               ]             }           }         }       },       {         \"title\": \"Average Latency\",         \"targets\": [           {             \"expr\": \"cache_avg_latency_us / 1000\"           }         ],         \"fieldConfig\": {           \"defaults\": {             \"unit\": \"ms\"           }         }       }     ]   } }     Key Metrics to Monitor   Business Metrics                  Metric       Target       Alert &gt;                       Cache Hit Rate       &gt; 70%       &lt; 30%                 Error Rate       &lt; 1%       &gt; 5%                 P99 Latency       &lt; 50ms       &gt; 100ms                 Throughput       Baseline       Drop &gt; 20%           System Metrics                  Metric       Target       Alert &gt;                       Connection Pool Active       &lt; max       == max (5 min)                 Memory Usage       Stable       +50% from baseline                 Cache Size       Bounded       Growing unbounded                 Evictions       None       &gt; 0/min             On-Call Runbook   If Hit Rate Drops Below 30%   Severity: Warning  Time to investigate: 15 minutes      Check if this is normal:     SELECT avg(hit_rate) FROM metrics WHERE time &gt; now() - interval '1 hour' -- Is this actually a change?           Check for recent changes:            Any code deployments?       Any data migrations?       Any schema changes?           Investigate root cause:            Hit rate low for specific entity types? (e.g., only users, not products)       Did user traffic pattern change? (new traffic type not being cached)       Is TTL too short? (redis-cli TTL \"sample-key\")       Are cache keys non-deterministic?           Remediate:            If TTL too short: Increase TTL       If key generation wrong: Fix code and redeploy       If traffic pattern changed: Update caching strategy           If Error Rate Exceeds 5%   Severity: Critical  Time to respond: &lt; 5 minutes      Check if backend is up:     redis-cli ping # Response: PONG (good) # Response: error (bad - Redis down)           Check network connectivity:     nc -zv localhost 6379 redis-cli --latency           Check connection pool:     error!(\"Active: {}, Waiting: {}\", active_conns, waiting_reqs);           Actions:            If backend down: Restart it       If pool exhausted: Increase pool size       If network issue: Check firewall/connectivity       If errors persisting: Failover to backup cache           If Latency Exceeds 100ms   Severity: Warning  Time to investigate: 10 minutes      Check if this is sustained:     Is p99 consistently &gt; 100ms for &gt; 5 min? Or is this a temporary spike?           Check network latency:     redis-cli --latency # &lt; 1ms (good) # &gt; 10ms (network issue)           Check backend load:     redis-cli INFO stats # instantaneous_ops_per_sec: ? # Is Redis CPU bound?           Actions:            Increase connection pool size       Check for slow queries on database       If sustained, scale up Redis       Review timeout configuration             Monitoring Best Practices   DO ✅      Monitor 4 golden signals     metrics.record_hit(latency);      // Latency + Traffic metrics.record_miss(latency);     // Latency + Traffic metrics.record_error(latency);    // Errors + Latency           Alert on actionable metrics            Hit rate &lt; 30% → Investigate caching strategy       Error rate &gt; 5% → Backend issue       Latency p99 &gt; 100ms → Performance issue           Baseline your metrics     Healthy state: 80% hit rate, &lt; 1% errors, &lt; 50ms p99 Use this for anomaly detection           Include context in metrics     // Label by entity type cache_hits{entity_type=\"user\"} 1000 cache_hits{entity_type=\"product\"} 500     // Find which type has low hit rate           DON’T ❌      Alert on every metric change     // Bad: Too many alerts if hit_rate != yesterday_hit_rate {     alert!(); }  // Good: Alert on significant change if hit_rate &lt; baseline * 0.7 {  // 30% drop     alert!(); }           Ignore “normal” errors ```rust // These are expected, don’t alert:            Cache miss (expected)       Serialization error on schema change (expected after deploy)       Timeout during network partition (expected) ```           Set thresholds without baselines     // Don't guess: 30% error rate good or bad? // Establish baseline first let baseline_error_rate = 0.01;  // 1% let threshold = baseline_error_rate * 5;  // Alert at 5x normal           Forget about cardinality     // Bad: Unbounded labels (infinite cardinality) cache_hits{user_id=\"123\"} 1 cache_hits{user_id=\"456\"} 1 // Cardinality explodes!  // Good: Fixed dimensions cache_hits{entity_type=\"user\"} 2             Troubleshooting Metrics   “All metrics show zeros”   Cause: Metrics not being recorded  Solution:  // Verify code is calling record_hit/record_miss match cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh) {     Ok(_) =&gt; metrics.record_hit(latency),  // Add this     Err(e) =&gt; metrics.record_error(latency),  // Add this }   “Hit rate doesn’t match expected”   Cause: Metrics counting different things  Solution:  // Verify what counts as hit vs miss // Cache hit: Entry exists in cache, returned metrics.record_hit(latency);  // Cache miss: Entry not in cache, fetched from DB metrics.record_miss(latency);  // Error: Cache operation failed metrics.record_error(latency);   “Prometheus shows NaN for hit_rate”   Cause: Division by zero (no operations yet)  Solution:  pub fn hit_rate(&amp;self) -&gt; f64 {     let total = self.hits.load(Ordering::Relaxed)                + self.misses.load(Ordering::Relaxed);     if total == 0 {         return 0.0;  // Return 0, not NaN     }     // ... calculation }     Monitoring Checklist   Before production:      Metrics struct implemented   All cache operations instrumented (hit/miss/error)   Latency measurement accurate   Prometheus scrape endpoint working   Prometheus config pointing to correct endpoint   Alert rules configured   Grafana dashboard created   Baseline metrics established   On-call runbook written   Team trained on runbook     Next Steps      Read Error Handling guide for handling metric anomalies   Check Troubleshooting guide for diagnosis patterns   Review Backends guide for backend-specific metrics     See Also      Troubleshooting Guide — Using metrics to diagnose issues   Error Handling — Understanding error metrics   Performance Guide — Benchmarking vs. production metrics   Here are shortened versions cutting marketing jargon and focusing on actionable content:   docs/guides/errir-handling.md (Target: 1,200 words)   Keep:   Error categories (1-4) Recovery patterns (1-2 essential ones) Testing approaches DO/DON’T checklist   Remove:   Verbose “Overview” marketing speak Redundant error flow diagrams Duplicate examples across patterns     docs/guides/troubleshooting.md (Target: 1,100 words)   Keep:   4 common issues (concise root causes + solutions)  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/guides/monitoring/",
        "teaser": null
      },{
        "title": "Performance Guide",
        "excerpt":"Performance Guide   Comprehensive guide to benchmarking, optimization, and production tuning for cache-kit.     Overview   This guide covers:     Benchmarking — Measuring and comparing cache performance   Pool Optimization — Tuning connection pools for Redis and Memcached   Production Monitoring — Tracking performance in production   Troubleshooting — Diagnosing performance issues     Benchmarking   Running Benchmarks   cache-kit uses Criterion for statistically rigorous performance benchmarking.   # Run all benchmarks and view results make perf  # Run backend-specific benchmarks (requires services running) cargo bench --bench redis_benchmark --features redis cargo bench --bench memcached_benchmark --features memcached  # Save current performance as baseline make perf-save  # Compare against saved baseline (detect regressions) make perf-diff   What Gets Benchmarked      InMemory Backend — Cache operations (set, get, delete) with throughput metrics   Redis Backend — Network-backed operations, batch operations, connection pooling (requires Redis running)   Memcached Backend — Network-backed operations, batch operations, binary protocol (requires Memcached running)   CacheExpander — Full cache lifecycle (hit/miss paths) with throughput metrics   Serialization — Bincode performance across payload sizes with throughput metrics   Understanding Criterion Output   inmemory_backend/get_hit/1000                         time:   [30.456 ns 30.789 ns 31.123 ns]                                  ^^^^^^^^  ^^^^^^^^  ^^^^^^^^                                  lower     median    upper (95% confidence)      Median: Most reliable performance metric   Confidence interval: Range where true performance likely falls (95% certain)   Outliers: Anomalies removed automatically   Baseline Comparison   inmemory_backend/get_hit/1000                         time:   [30.123 ns 30.456 ns 30.789 ns]                         change: [-40.234% -38.567% -36.891%] (p = 0.00 &lt; 0.05)                         Performance has improved.      change: Performance difference vs baseline   p-value: Statistical significance (p &lt; 0.05 = real change, not noise)   Verdict: “improved”, “regressed”, or “no change”   What is a Baseline?   A baseline is a saved snapshot of benchmark performance used for comparison:   # Step 1: Before optimization - save baseline make perf-save # Saves to: target/criterion/*/main/  # Step 2: Make code changes vim src/backend/inmemory.rs  # Step 3: Compare new performance make perf-diff # Shows: \"40% faster!\" or \"5% slower (regression!)\"   Baselines are stored in target/criterion/&lt;benchmark-name&gt;/main/. The name “main” is just a label — you can use others:   cargo bench -- --save-baseline v0.3.0 cargo bench -- --baseline v0.3.0   Performance Variance   Benchmarks won’t be exactly identical across runs due to:     CPU frequency scaling   Background processes   OS scheduler   Cache state   Typical variation: ±1-5%   Criterion uses statistical analysis to determine if changes are real:     &lt; 2% difference: “No change detected” (within noise)   &gt; 5% difference + p &lt; 0.05: “Performance changed” (statistically significant)     Expected Performance   InMemory Backend (In-Process)                  Benchmark       Typical Time       Throughput                       inmemory_backend/get_miss       15-20 ns       50-66M ops/sec                 inmemory_backend/get_hit/1KB       30-50 ns       20-33M ops/sec, 20-33 GB/sec                 inmemory_backend/set/1KB       40-60 ns       16-25M ops/sec, 16-25 GB/sec                 cache_expander/refresh_hit/1KB       100-200 ns       5-10M ops/sec, 5-10 GB/sec                 cache_expander/refresh_miss/1KB       1-5 µs       200K-1M ops/sec, 200MB-1GB/sec                 serialization/serialize/1KB       50-100 ns       10-20M ops/sec, 10-20 GB/sec           Redis Backend (Network-Backed)                  Benchmark       Typical Time       Throughput                       redis_backend/get_miss       200-500 µs       2K-5K ops/sec                 redis_backend/get_hit/1KB       200-600 µs       1.6K-5K ops/sec, 1.6-5 MB/sec                 redis_backend/set/1KB       300.9.0 µs       1.2K-3.3K ops/sec, 1.2-3.3 MB/sec                 redis_batch_ops/mget/batch_10_size_1000       300-1000 µs       10K-33K ops/sec, 10-33 MB/sec                 redis_connection_pool/health_check       100-300 µs       3.3K-10K ops/sec           Memcached Backend (Network-Backed)                  Benchmark       Typical Time       Throughput                       memcached_backend/get_miss       150-400 µs       2.5K-6.6K ops/sec                 memcached_backend/get_hit/1KB       150-500 µs       2K-6.6K ops/sec, 2-6.6 MB/sec                 memcached_backend/set/1KB       200-600 µs       1.6K-5K ops/sec, 1.6-5 MB/sec                 memcached_batch_ops/mget/batch_10_size_1000       250.9.0 µs       12.5K-40K ops/sec, 12.5-40 MB/sec                 memcached_protocol/health_check       100-250 µs       4K-10K ops/sec           Note: Network-backed benchmarks are ~1000x slower than in-memory due to network latency. Times vary based on network conditions and service configuration.     Connection Pool Optimization   Research Summary   Optimal pool size: 16 connections (for 8-core systems)      Improvement: 49-53% latency reduction vs. pool size 10   Outlier reduction: 22% → 8% (2.75x reduction in contention)   Formula: (CPU cores × 2) + 1   Benchmark Results                  Operation       Pool=10       Pool=16       Pool=32       Best                       SET/100       809 µs       412 µs       381 µs       16 ✅                 SET/1KB       584 µs       383 µs       388 µs       16 ✅                 SET/10KB       540 µs       524 µs       525 µs       tie                 GET/100       385 µs       395 µs       383 µs       32                 GET/1KB       379 µs       386 µs       382 µs       10                 Outliers       22%       8%       12%       16 ✅           Verdict: Pool=16 provides best balance of latency and stability.   Why Pool Size Matters   Connection Queue Dynamics   Pool Size = 10 (Under-provisioned)  ┌─ Redis Server (1 connection limit) │ ├─ Active conn 1 ├─ Active conn 2 ├─ ... ├─ Active conn 10 │ └─ Queue: [req11, req12, ... req50]  ← 40 requests waiting!    Waiting = HIGH LATENCY &amp; OUTLIERS   Pool Size = 16 (Optimal)  ┌─ Redis Server │ ├─ Active conn 1-14 ├─ Available conn 15-16  ← buffer for burst traffic │ └─ Queue: [req45, req46...]  ← 6 requests waiting (acceptable)    Waiting = LOW LATENCY &amp; STABLE   Pool Size = 32 (Over-provisioned)  ├─ Active conn 1-14 ├─ Idle conn 15-32  ← WASTED MEMORY &amp; CPU │ └─ Queue: empty    Problem: Network becomes bottleneck, not pooling   Pool Sizing Formula   Cores: 8 (typical development system) Calculation: (8 × 2) + 1 = 17 ≈ 16  Recommended: 16 connections   Scaling to Other Hardware                  Cores       Formula       Recommended                       4       9       8-10                 8       17       16 (default)                 16       33       32                 32       65       64                 64       129       128           Default Configuration   cache-kit defaults to pool size 16 for optimal performance on typical systems:   // src/backend/redis.rs const DEFAULT_POOL_SIZE: u32 = 16;  // Optimized default  // src/backend/memcached.rs const DEFAULT_POOL_SIZE: u32 = 16;  // Optimized default   Custom Pool Sizing   Override defaults using environment variables or configuration:   # High-concurrency service REDIS_POOL_SIZE=32 cargo run  # Low-traffic service MEMCACHED_POOL_SIZE=8 cargo run   Or in code:   use cache_kit::backend::{RedisBackend, RedisConfig}; use std::time::Duration;  let config = RedisConfig {     host: \"localhost\".to_string(),     port: 6379,     pool_size: 32,  // Override default     connection_timeout: Duration::from_secs(5),     username: None,     password: None,     database: 0, };  let backend = RedisBackend::new(config)?;     Production Monitoring   Key Metrics to Track   1. Connection Pool Utilization   Ideal: 70-90% of pool connections in use Too low (&lt; 50%): Over-provisioned, waste memory Too high (&gt; 95%): Under-provisioned, requests queue up   2. Request Latency Percentiles   p50: &lt; 1ms (median) p95: &lt; 5ms (95th percentile) p99: &lt; 20ms (99th percentile - acceptable spikes)   3. Connection Wait Time   Monitor queue depth: should be near zero Spikes indicate under-provisioning   Tuning in Production   If you see high p99 latency:  # Increase pool size REDIS_POOL_SIZE=$(($(nproc) * 2 + 1)) cargo run   If you see memory pressure:  # Decrease pool size REDIS_POOL_SIZE=$(($(nproc) * 1)) cargo run   Application Metrics   Implement cache metrics in your application:   use std::sync::atomic::{AtomicU64, Ordering};  struct CacheMetrics {     hits: AtomicU64,     misses: AtomicU64,     errors: AtomicU64, }  impl CacheMetrics {     fn hit_rate(&amp;self) -&gt; f64 {         let hits = self.hits.load(Ordering::Relaxed);         let misses = self.misses.load(Ordering::Relaxed);         if hits + misses == 0 {             return 0.0;         }         hits as f64 / (hits + misses) as f64     } }     Benchmark Prerequisites   Redis Benchmarks   # Start Redis (Docker recommended) docker run -d -p 6379:6379 redis:latest  # Or use system Redis redis-server  # Run benchmarks cargo bench --bench redis_benchmark --features redis   Memcached Benchmarks   # Start Memcached (Docker recommended) docker run -d -p 11211:11211 memcached:latest  # Or use system Memcached memcached -p 11211  # Run benchmarks cargo bench --bench memcached_benchmark --features memcached   Using Docker Compose   # Start all services (from project root) make up  # Run all benchmarks make perf  # Stop services make down     Advanced Benchmarking   Custom Baselines   # Save version-specific baselines cargo bench -- --save-baseline v0.3.0 cargo bench -- --save-baseline before-optimization  # Compare against specific baseline cargo bench -- --baseline v0.3.0   Run Specific Benchmarks   # Run only InMemory benchmarks cargo bench -- inmemory_backend  # Run only cache expander cargo bench -- cache_expander  # Run single benchmark cargo bench -- inmemory_backend/get_miss  # Run specific backend benchmark suites cargo bench --bench redis_benchmark --features redis -- redis_backend cargo bench --bench memcached_benchmark --features memcached -- memcached_backend  # Run specific Redis benchmark group cargo bench --bench redis_benchmark --features redis -- redis_batch_ops   Using Feature Flags   # Test specific backends make test FEATURES=\"--features redis\" make test FEATURES=\"--all-features\"  # Build with features make build FEATURES=\"--features memcached\"  # Benchmark specific backends cargo bench --bench redis_benchmark --features redis cargo bench --bench memcached_benchmark --features memcached     CI Integration   GitHub Actions Example   # In .github/workflows/ci.yml - name: Run benchmarks   run: make perf-save  - name: Check for regressions   run: make perf-diff   View HTML Report   After running make perf, open:  target/criterion/report/index.html   Shows:     Performance violin plots   Statistical analysis   Regression detection   Historical comparisons     Troubleshooting   “No baseline found”   Solution: Run make perf-save first to create a baseline.   “Large variance in results”   Causes:     Background processes consuming CPU   Running on battery power (CPU throttling)   Thermal throttling   Solutions:     Close other applications   Run on AC power (not battery)   Ensure adequate cooling   Run benchmarks multiple times and check consistency   “Benchmarks take too long”   This is normal! Criterion runs 100+ iterations for statistical accuracy.   Typical runtime: 5-10 minutes for full suite   To speed up:  # Run specific benchmark groups cargo bench -- inmemory_backend  # Or reduce sample size (less accurate) cargo bench -- --sample-size 10   High Latency in Network Backends   Checklist:     Redis/Memcached services running?   Network connectivity OK?   Connection pool sized appropriately?   Check service logs for errors   Verify pool configuration matches CPU cores     Performance Optimization Summary                  Metric       Pool=10 → Pool=16                       SET latency (100B)       809 µs → 412 µs (-49%)                 SET latency (1KB)       584 µs → 383 µs (-34%)                 Outlier reduction       22% → 8% (-64%)                 Consistency       Poor → Excellent                 Tail latency (p99)       High → Low             References      Criterion documentation   Deadpool documentation — Pool sizing recommendations   Redis best practices — Connection pooling   Hardware utilization: CPU core × 2 rule     See Also      Testing Guide — Unit and integration testing strategies   Cache Backend Support — Backend configuration details   Contributing Guide — Code standards and submission process     Status: ✅ Benchmarking and optimization guidelines verified with Criterion on 8-core systems.  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/guides/performance/",
        "teaser": null
      },{
        "title": "Testing Guide",
        "excerpt":"Testing Guide - Quick Start   Makefile Commands (Recommended)   Quick Start   # Start services (Redis + Memcached) make up  # Run all tests make test-all  # Stop services make down   Common Commands   make unittest          # Unit tests only make integration-test  # Integration tests make redis-test        # Redis integration tests (requires 'make up') make check            # Format + lint + unit tests make help             # Show all commands   Infrastructure   make up      # Start Redis + Memcached make down    # Stop all services make ps      # Show running services make logs    # Show service logs make clean   # Stop and remove containers     Testing Guide   Comprehensive testing strategies for the cache framework and your custom implementations.     Table of Contents      Running Tests   Unit Testing Strategies   Testing Custom Backends   Testing Custom Feeders   Testing Repositories   Integration Testing   Performance Testing   Mocking &amp; Test Doubles   Common Testing Patterns     Running Tests   Run All Tests   cargo test --all-features   Run Tests for Specific Feature   cargo test --features inmemory cargo test --features redis cargo test --features memcached   Run Tests with Logging   RUST_LOG=debug cargo test RUST_LOG=cache=trace cargo test   Run Specific Test   cargo test test_cache_hit -- --nocapture   Run Tests in Parallel or Sequentially   # Parallel (default) cargo test --all-features  # Sequential (useful for debugging) cargo test --all-features -- --test-threads=1   View Test Output   cargo test -- --nocapture     Unit Testing Strategies   Testing CacheEntity   #[cfg(test)] mod tests {     use super::*;     use serde_json;      #[test]     fn test_cache_key_generation() {         let entity = Employment {             id: \"emp_123\".to_string(),             employer: \"Acme\".to_string(),         };          let key = entity.cache_key();         assert_eq!(key.to_string(), \"emp_123\");     }      #[test]     fn test_cache_prefix() {         assert_eq!(Employment::cache_prefix(), \"employment\");     }      #[test]     fn test_serialization() {         let entity = Employment {             id: \"emp_123\".to_string(),             employer: \"Acme\".to_string(),         };          let bytes = entity.serialize_for_cache().unwrap();         let deserialized = Employment::deserialize_from_cache(&amp;bytes).unwrap();                  assert_eq!(entity.id, deserialized.id);         assert_eq!(entity.employer, deserialized.employer);     }      #[test]     fn test_validation() {         let valid = Employment {             id: \"emp_123\".to_string(),             employer: \"Acme\".to_string(),         };          assert!(valid.validate().is_ok());          let invalid = Employment {             id: \"\".to_string(), // Empty ID             employer: \"Acme\".to_string(),         };          assert!(invalid.validate().is_err());     } }   Testing Cache Strategies   #[test] fn test_cache_strategy_fresh() {     // Fresh strategy should only use cache, not DB     let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);      // Pre-populate cache     // ... then verify Fresh returns cached data without touching DB }  #[test] fn test_cache_strategy_refresh() {     // Refresh should try cache, fallback to DB     let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);      // First call: cache miss -&gt; DB fetch     // Second call: cache hit -&gt; use cached value }  #[test] fn test_cache_strategy_invalidate() {     // Invalidate should clear cache and fetch fresh     // Verify old data is not returned }  #[test] fn test_cache_strategy_bypass() {     // Bypass should skip cache entirely     // Verify it goes directly to DB }     Testing Custom Backends   Backend Interface Testing   #[cfg(test)] mod backend_tests {     use super::*;     use std::time::Duration;      // Generic test function for any backend     fn test_backend_get_set&lt;B: CacheBackend&gt;(mut backend: B) {         let key = \"test_key\";         let value = b\"test_value\".to_vec();          // Set         assert!(backend.set(key, value.clone(), None).is_ok());          // Get         let retrieved = backend.get(key).unwrap();         assert_eq!(retrieved, Some(value));     }      fn test_backend_delete&lt;B: CacheBackend&gt;(mut backend: B) {         let key = \"test_key\";         let value = b\"test_value\".to_vec();          backend.set(key, value, None).unwrap();         backend.delete(key).unwrap();          let result = backend.get(key).unwrap();         assert_eq!(result, None);     }      fn test_backend_exists&lt;B: CacheBackend&gt;(mut backend: B) {         let key = \"test_key\";          // Should not exist initially         assert!(!backend.exists(key).unwrap());          // After set         backend.set(key, b\"value\".to_vec(), None).unwrap();         assert!(backend.exists(key).unwrap());          // After delete         backend.delete(key).unwrap();         assert!(!backend.exists(key).unwrap());     }      fn test_backend_ttl_expiration&lt;B: CacheBackend&gt;(mut backend: B) {         let key = \"temp_key\";         let short_ttl = Duration::from_millis(100);          backend.set(key, b\"value\".to_vec(), Some(short_ttl)).unwrap();         assert!(backend.exists(key).unwrap());          // Wait for expiration         std::thread::sleep(Duration::from_millis(150));         assert!(!backend.exists(key).unwrap());     }      fn test_backend_clear_all&lt;B: CacheBackend&gt;(mut backend: B) {         backend.set(\"key1\", b\"value1\".to_vec(), None).unwrap();         backend.set(\"key2\", b\"value2\".to_vec(), None).unwrap();          backend.clear_all().unwrap();          assert!(backend.get(\"key1\").unwrap().is_none());         assert!(backend.get(\"key2\").unwrap().is_none());     }      fn test_backend_health_check&lt;B: CacheBackend&gt;(mut backend: B) {         let health = backend.health_check().unwrap();         assert!(health);     }      // Batch operation tests     fn test_backend_mget&lt;B: CacheBackend&gt;(mut backend: B) {         backend.set(\"key1\", b\"value1\".to_vec(), None).unwrap();         backend.set(\"key2\", b\"value2\".to_vec(), None).unwrap();          let results = backend.mget(&amp;[\"key1\", \"key2\"]).unwrap();                  assert_eq!(results.len(), 2);         assert_eq!(results[0], Some(b\"value1\".to_vec()));         assert_eq!(results[1], Some(b\"value2\".to_vec()));     }      fn test_backend_mdelete&lt;B: CacheBackend&gt;(mut backend: B) {         backend.set(\"key1\", b\"value1\".to_vec(), None).unwrap();         backend.set(\"key2\", b\"value2\".to_vec(), None).unwrap();          backend.mdelete(&amp;[\"key1\", \"key2\"]).unwrap();          assert!(backend.get(\"key1\").unwrap().is_none());         assert!(backend.get(\"key2\").unwrap().is_none());     }      // For InMemoryBackend     #[test]     fn test_inmemory_backend() {         let backend = InMemoryBackend::new();         test_backend_get_set(backend.clone());         test_backend_delete(backend.clone());         test_backend_exists(backend.clone());         test_backend_ttl_expiration(backend.clone());         test_backend_clear_all(backend.clone());         test_backend_health_check(backend.clone());         test_backend_mget(backend.clone());         test_backend_mdelete(backend);     } }   Backend-Specific Tests   #[test] #[cfg(feature = \"redis\")] fn test_redis_backend_connection() {     let config = RedisConfig {         host: \"localhost\".to_string(),         port: 6379,         ..Default::default()     };      let backend = RedisBackend::new(config);     assert!(backend.is_ok()); }  #[test] #[cfg(feature = \"memcached\")] fn test_memcached_backend_multi_server() {     let config = MemcachedConfig {         servers: vec![             \"localhost:11211\".to_string(),             \"localhost:11212\".to_string(),         ],         ..Default::default()     };      let backend = MemcachedBackend::new(config);     assert!(backend.is_ok()); }     Testing Custom Feeders   #[cfg(test)] mod feeder_tests {     use super::*;      #[test]     fn test_feeder_entity_id() {         let mut feeder = EmploymentFeeder {             id: \"emp_123\".to_string(),             employment: None,         };          assert_eq!(feeder.entity_id(), \"emp_123\");     }      #[test]     fn test_feeder_feed_some() {         let mut feeder = EmploymentFeeder {             id: \"emp_123\".to_string(),             employment: None,         };          let entity = Employment {             id: \"emp_123\".to_string(),             employer: \"Acme\".to_string(),         };          feeder.feed(Some(entity.clone()));                  assert!(feeder.employment.is_some());         assert_eq!(feeder.employment.unwrap().employer, \"Acme\");     }      #[test]     fn test_feeder_feed_none() {         let mut feeder = EmploymentFeeder {             id: \"emp_123\".to_string(),             employment: Some(Employment {                 id: \"emp_456\".to_string(),                 employer: \"OldCorp\".to_string(),             }),         };          feeder.feed(None);         assert!(feeder.employment.is_none());     }      #[test]     fn test_generic_feeder() {         let mut feeder = GenericFeeder::&lt;Employment&gt;::new(\"emp_123\".to_string());                  let entity = Employment {             id: \"emp_123\".to_string(),             employer: \"Acme\".to_string(),         };          feeder.feed(Some(entity));         assert!(feeder.entity.is_some());     } }     Testing Repositories   Mock Repository for Testing   pub struct MockRepository&lt;T&gt; {     data: std::collections::HashMap&lt;String, T&gt;, }  impl&lt;T: Clone&gt; MockRepository&lt;T&gt; {     pub fn new() -&gt; Self {         MockRepository {             data: std::collections::HashMap::new(),         }     }      pub fn insert(&amp;mut self, id: String, entity: T) {         self.data.insert(id, entity);     } }  impl&lt;T: Clone + 'static&gt; DataRepository&lt;T&gt; for MockRepository&lt;T&gt; {     fn fetch_by_id(&amp;self, id: &amp;str) -&gt; Result&lt;Option&lt;T&gt;&gt; {         Ok(self.data.get(id).cloned())     } }  #[test] fn test_with_mock_repository() {     let mut repo = MockRepository::new();     repo.insert(\"emp_123\".to_string(), Employment {         id: \"emp_123\".to_string(),         employer: \"Acme\".to_string(),     });      let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);      let mut feeder = GenericFeeder::&lt;Employment&gt;::new(\"emp_123\".to_string());     expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh).unwrap();      assert!(feeder.entity.is_some()); }   Repository with In-Memory Database   #[test] fn test_repository_with_sqlx() {     let runtime = tokio::runtime::Runtime::new().unwrap();          runtime.block_on(async {         let database_url = \"sqlite::memory:\";         let pool = sqlx::SqlitePool::connect(database_url).await.unwrap();          // Create table         sqlx::query(\"CREATE TABLE employment (id TEXT, employer TEXT)\")             .execute(&amp;pool)             .await             .unwrap();          // Insert test data         sqlx::query(\"INSERT INTO employment VALUES (?, ?)\")             .bind(\"emp_123\")             .bind(\"Acme\")             .execute(&amp;pool)             .await             .unwrap();          // Test repository         let repo = EmploymentRepository { pool };         let result = repo.fetch_by_id(\"emp_123\").unwrap();                  assert!(result.is_some());         assert_eq!(result.unwrap().employer, \"Acme\");     }); }     Integration Testing   Full Cache Workflow   #[test] fn test_full_cache_workflow() {     // Setup     let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);     let mut repo = MockRepository::new();      // Seed repository     repo.insert(\"emp_123\".to_string(), Employment {         id: \"emp_123\".to_string(),         employer: \"Acme\".to_string(),     });      // First call: cache miss, DB hit     let mut feeder = GenericFeeder::&lt;Employment&gt;::new(\"emp_123\".to_string());     expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh).unwrap();     assert!(feeder.entity.is_some());      // Modify repository (should not affect cache)     repo.data.remove(\"emp_123\");      // Second call: cache hit (data still available)     let mut feeder2 = GenericFeeder::&lt;Employment&gt;::new(\"emp_123\".to_string());     expander.with(&amp;mut feeder2, &amp;repo, CacheStrategy::Fresh).unwrap();     assert!(feeder2.entity.is_some()); }   Multi-Entity Integration Test   #[test] fn test_multiple_entity_types() {     let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);          let mut employment_repo = MockRepository::new();     let mut borrower_repo = MockRepository::new();      // Cache Employment     let mut emp_feeder = GenericFeeder::&lt;Employment&gt;::new(\"emp_1\".to_string());     expander.with(&amp;mut emp_feeder, &amp;employment_repo, CacheStrategy::Refresh).ok();      // Cache Borrower     let mut bor_feeder = GenericFeeder::&lt;Borrower&gt;::new(\"bor_1\".to_string());     expander.with(&amp;mut bor_feeder, &amp;borrower_repo, CacheStrategy::Refresh).ok();      // Both should be cached independently     assert!(emp_feeder.entity.is_some());     assert!(bor_feeder.entity.is_some()); }     Performance Testing   For comprehensive performance benchmarking using Criterion, including baseline comparison, regression detection, and connection pool optimization, see the Performance Guide.   The Performance Guide covers:     Running benchmarks with Criterion (make perf)   Understanding statistical output and confidence intervals   Baseline comparison and regression detection   Expected performance metrics by backend   Connection pool sizing and optimization   Production monitoring and tuning   Quick Ad-Hoc Performance Checks   For simple performance checks in unit tests:   #[test] fn quick_perf_check_cache_hit() {     let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);     let repo = MockRepository::new();      let iterations = 10_000;     let start = std::time::Instant::now();      for i in 0..iterations {         let mut feeder = GenericFeeder::&lt;Employment&gt;::new(format!(\"emp_{}\", i));         let _ = expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Fresh);     }      let elapsed = start.elapsed();     let per_op = elapsed.as_nanos() / iterations;      println!(\"Cache hit latency: {} ns/op\", per_op);      // Sanity check (not statistically rigorous)     assert!(per_op &lt; 5000, \"Cache hit too slow: {} ns\", per_op); }   Note: For statistically rigorous benchmarking, use Criterion instead of manual timing. See Performance Guide.     Mocking &amp; Test Doubles   Mock CacheMetrics   pub struct MockMetrics {     pub hits: std::sync::Mutex&lt;Vec&lt;String&gt;&gt;,     pub misses: std::sync::Mutex&lt;Vec&lt;String&gt;&gt;,     pub errors: std::sync::Mutex&lt;Vec&lt;String&gt;&gt;, }  impl CacheMetrics for MockMetrics {     fn record_hit(&amp;self, key: &amp;str, _: Duration) {         self.hits.lock().unwrap().push(key.to_string());     }      fn record_miss(&amp;self, key: &amp;str, _: Duration) {         self.misses.lock().unwrap().push(key.to_string());     }      fn record_set(&amp;self, key: &amp;str, _: Duration) {         // ...     }      fn record_error(&amp;self, key: &amp;str, _: &amp;str) {         self.errors.lock().unwrap().push(key.to_string());     } }  #[test] fn test_metrics_recording() {     let metrics = MockMetrics {         hits: std::sync::Mutex::new(vec![]),         misses: std::sync::Mutex::new(vec![]),         errors: std::sync::Mutex::new(vec![]),     };      metrics.record_hit(\"emp_123\", Duration::from_millis(1));     assert_eq!(metrics.hits.lock().unwrap().len(), 1); }   Spy Backend   pub struct SpyBackend {     inner: InMemoryBackend,     pub get_calls: std::sync::Mutex&lt;Vec&lt;String&gt;&gt;,     pub set_calls: std::sync::Mutex&lt;Vec&lt;String&gt;&gt;, }  impl SpyBackend {     pub fn new() -&gt; Self {         SpyBackend {             inner: InMemoryBackend::new(),             get_calls: std::sync::Mutex::new(vec![]),             set_calls: std::sync::Mutex::new(vec![]),         }     } }  impl CacheBackend for SpyBackend {     fn get(&amp;mut self, key: &amp;str) -&gt; Result&lt;Option&lt;Vec&lt;u8&gt;&gt;&gt; {         self.get_calls.lock().unwrap().push(key.to_string());         self.inner.get(key)     }      fn set(&amp;mut self, key: &amp;str, value: Vec&lt;u8&gt;, ttl: Option&lt;Duration&gt;) -&gt; Result&lt;()&gt; {         self.set_calls.lock().unwrap().push(key.to_string());         self.inner.set(key, value, ttl)     }      // ... other methods ... }  #[test] fn test_backend_interactions() {     let mut backend = SpyBackend::new();     let mut expander = CacheExpander::new(backend.clone());      // ... perform cache operations ...      // Verify interactions     assert_eq!(backend.get_calls.lock().unwrap().len(), 2);     assert_eq!(backend.set_calls.lock().unwrap().len(), 1); }     Common Testing Patterns   Test Error Cases   #[test] fn test_cache_error_handling() {     // Test missing key     let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);     let repo = MockRepository::new(); // Empty repo      let mut feeder = GenericFeeder::&lt;Employment&gt;::new(\"nonexistent\".to_string());     let result = expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Fresh);      // Should handle gracefully     assert!(feeder.entity.is_none()); }  #[test] fn test_serialization_error() {     #[derive(Clone)]     struct NonSerializable;      // Implementation would fail to serialize     // Test error handling path }   Parameterized Tests   #[test] fn test_all_strategies() {     for strategy in [         CacheStrategy::Fresh,         CacheStrategy::Refresh,         CacheStrategy::Invalidate,         CacheStrategy::Bypass,     ] {         // Test each strategy         assert_strategy_works(&amp;strategy);     } }  fn assert_strategy_works(strategy: &amp;CacheStrategy) {     // Strategy-specific assertions }   Test State Cleanup   struct TestFixture {     backend: InMemoryBackend,     expander: CacheExpander&lt;InMemoryBackend&gt;, }  impl TestFixture {     fn new() -&gt; Self {         let backend = InMemoryBackend::new();         let expander = CacheExpander::new(backend.clone());         TestFixture { backend, expander }     }      fn cleanup(&amp;mut self) {         // Clear state between tests         self.backend.clear_all().ok();     } }  impl Drop for TestFixture {     fn drop(&amp;mut self) {         self.cleanup();     } }  #[test] fn test_with_fixture() {     let mut fixture = TestFixture::new();     // Use fixture...     // Cleanup happens automatically }     Best Practices      Test behavior, not implementation — Test what the cache does, not how it does it   Use descriptive test names — Name tests to describe what they verify   Keep tests independent — Each test should be runnable standalone   Use fixtures for setup — Reduce duplication with test helpers   Test error paths — Don’t just test the happy path   Mock external dependencies — Use mock repositories, not real databases   Benchmark-critical paths — Measure cache hit/miss performance   Test thread safety — Verify Send + Sync implementation with concurrent tests     See Also      CONTRIBUTING.md — Testing your extensions   EXAMPLES.md — Example implementations   README.md — Quick start  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/guides/testing/",
        "teaser": null
      },{
        "title": "Production Troubleshooting",
        "excerpt":"Production Troubleshooting Guide   Diagnose and resolve cache-kit issues in production environments.     Overview   This guide covers the most common cache-kit issues, how to diagnose them, and how to fix them.   Diagnostic Workflow   Issue Observed     ↓ Identify Category (Connection? Performance? Data?)     ↓ Gather Logs &amp; Metrics     ↓ Check Backend Health     ↓ Apply Fix     ↓ Verify Resolution   Tools You’ll Need   # Redis diagnosis redis-cli redis-cli --latency redis-cli --stat  # Memcached diagnosis echo \"stats\" | nc localhost 11211 memcached-tool localhost:11211  # Application logs grep \"cache\" app.log | grep \"error\"  # System metrics top vmstat netstat     Common Issues &amp; Solutions   Issue 1: Low Cache Hit Rate (&lt; 30%)   Symptoms:     Hit rate stuck at 10-20%   Cache size not growing   High database load despite caching   Possible Causes:     Keys are not being reused (different key each time)   TTL is too short (entries expire quickly)   Cache keys are non-deterministic   Cache is being cleared unexpectedly   New users/data not being cached   Diagnosis Steps   Step 1: Check hit/miss rates  pub struct CacheMetrics {     hits: AtomicU64,     misses: AtomicU64, }  impl CacheMetrics {     pub fn hit_rate(&amp;self) -&gt; f64 {         let h = self.hits.load(Ordering::Relaxed) as f64;         let m = self.misses.load(Ordering::Relaxed) as f64;         h / (h + m)     } }  // In your metrics endpoint println!(\"Cache hit rate: {:.1}%\", metrics.hit_rate() * 0.9.0);   Step 2: Verify cache keys are deterministic  // ✅ Good: Same input always produces same key impl CacheEntity for User {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key {         self.id.clone()  // Deterministic     } }  // ❌ Bad: Different keys for same entity impl CacheEntity for User {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key {         // WRONG: Creates new key each time!         format!(\"{}:{}\", self.id, SystemTime::now().timestamp())     } }   Step 3: Check TTL configuration  # Redis: Check expiration times redis-cli TTL \"user:123\" # Output: 3600 (seconds remaining) # Output: -1 (no expiration set - cache forever!) # Output: -2 (key doesn't exist)  # If TTL is -1 or very short, that's your problem   Step 4: Verify cache strategy  // If using CacheStrategy::Fresh instead of Refresh, you miss database writes // ❌ Wrong: Always cache-only expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Fresh)?;  // ✅ Correct: Cache with database fallback expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)?;   Solutions   Solution 1A: Set appropriate TTL  // Before: No TTL let expander = CacheExpander::new(backend);  // After: 1-hour TTL for user data let expander = CacheExpander::builder()     .with_backend(backend)     .with_ttl(Duration::from_secs(3600))     .build();   Solution 1B: Use Refresh strategy  // Ensure using Refresh, not Fresh expander.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)?; //              ↑ Try cache first, fallback to DB if miss   Solution 1C: Verify key uniqueness  // Log all cache keys to find issues impl CacheEntity for User {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key {         let key = self.id.clone();         debug!(\"Cache key for user: {}\", key);  // Add this         key     } }  // Check logs for duplicate/varying keys grep \"Cache key for user\" app.log | sort | uniq -c   Solution 1D: Check for cache invalidation logic  // Verify you're not over-invalidating // This is correct: Invalidate on write pub fn update_user(&amp;self, user: &amp;User) -&gt; Result&lt;()&gt; {     // 1. Update database     self.repo.update(user)?;          // 2. Invalidate OLD version     let mut feeder = UserFeeder { id: user.id.clone(), user: None };     self.cache.with(&amp;mut feeder, &amp;self.repo, CacheStrategy::Invalidate)?;          // 3. Cache NEW version     let mut feeder = UserFeeder { id: user.id.clone(), user: Some(user.clone()) };     self.cache.with(&amp;mut feeder, &amp;self.repo, CacheStrategy::Refresh)?;          Ok(()) }   Issue 2: Backend Connection Timeouts   Symptoms:     Request timeouts after N milliseconds   “Connection refused” errors   Pool exhaustion errors   p99 latency spikes   Possible Causes:     Backend (Redis/Memcached) is down   Network connectivity issue   Connection pool size is too small   Timeout is set too aggressively   Diagnosis Steps   Step 1: Verify backend is running  # Redis redis-cli ping # Response: PONG (good) # Response: (error) ERR... (bad - Redis down)  # Memcached echo \"stats\" | nc -w 1 localhost 11211 # Response: STAT... (good) # No response or error (bad - Memcached down)   Step 2: Check network connectivity  # Test network path nc -zv localhost 6379 # Connected to localhost port 6379 (good) # Connection refused (network issue)  # Check latency redis-cli --latency # Typical latency: &lt; 1ms (good) # Typical latency: &gt; 10ms (slow network)   Step 3: Examine connection pool status  // Monitor pool metrics pub struct PoolMetrics {     active_connections: AtomicU64,     waiting_requests: AtomicU64, }  // Log pool status on errors match cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh) {     Ok(_) =&gt; {},     Err(e) if e.to_string().contains(\"pool\") =&gt; {         error!(             \"Pool exhausted: {} active, {} waiting\",             metrics.active_connections.load(Ordering::Relaxed),             metrics.waiting_requests.load(Ordering::Relaxed)         );     }     Err(e) =&gt; error!(\"Cache error: {}\", e), }   Step 4: Check timeout configuration  // Current timeout let config = RedisConfig {     connection_timeout: Duration::from_secs(5),     // Is this too short for your network? };   Solutions   Solution 2A: Restart backend  # Redis redis-cli shutdown redis-server  # Or with Docker docker restart redis_container   Solution 2B: Increase pool size  // Before: Small pool let config = RedisConfig {     pool_size: 10,     ..Default::default() };  // After: Formula (CPU_cores × 2) + 1 let cores = num_cpus::get(); let config = RedisConfig {     pool_size: (cores * 2 + 1) as u32,     ..Default::default() };   Solution 2C: Increase timeout  // Before: Very strict timeout let config = RedisConfig {     connection_timeout: Duration::from_secs(1),     ..Default::default() };  // After: More realistic let config = RedisConfig {     connection_timeout: Duration::from_secs(10),     ..Default::default() };   Solution 2D: Add circuit breaker  // See Error Handling guide for circuit breaker implementation // This prevents cascading failures when backend is slow let breaker = CircuitBreaker::new(5, Duration::from_secs(30));     Issue 3: High Memory Usage   Symptoms:     Cache backend consuming GB of RAM   OOM killer triggering   Eviction errors from backend   Request latency increasing   Possible Causes:     Entries are too large (whole objects)   TTL not set (entries never expire)   Too many unique keys (unbounded growth)   No eviction policy configured   Memory leak in application   Diagnosis Steps   Step 1: Check Redis memory usage  redis-cli INFO memory # Output: # used_memory_human:2.5G  ← How much is being used # maxmemory:3G            ← Maximum allowed # evicted_keys:1000       ← Keys removed due to eviction   Step 2: Estimate entry size  redis-cli --bigkeys # Output: # Scanning database... # [Hash] \"employment:123\" -&gt; 512 bytes # [String] \"user:456\" -&gt; 128 bytes  # If entries are 512+ bytes, consider smaller DTOs   Step 3: Analyze key count  redis-cli DBSIZE # Output: 5000000 # With 1KB entries: 5GB of data # Check if all keys are needed   Step 4: Check eviction policy  redis-cli CONFIG GET maxmemory-policy # Output: \"allkeys-lru\" (good - evicts least recently used) # Output: \"no-eviction\" (bad - rejects new writes when full)   Solutions   Solution 3A: Set appropriate TTL  // Before: Cache forever let expander = CacheExpander::new(backend);  // After: 1-hour TTL for users, 1-day for products let user_cache = CacheExpander::builder()     .with_backend(redis_backend)     .with_ttl(Duration::from_secs(3600))  // 1 hour     .build();  let product_cache = CacheExpander::builder()     .with_backend(redis_backend)     .with_ttl(Duration::from_secs(86400))  // 1 day     .build();   Solution 3B: Reduce entry size  // Before: Cache entire User with all fields #[derive(Serialize, Deserialize)] struct CachedUser {     id: String,     name: String,     email: String,     password_hash: String,    // Unnecessary in cache     profile_picture: Vec&lt;u8&gt;, // Too large     bio: String,     preferences: Vec&lt;String&gt;, // Heavy }  // After: Cache only needed fields #[derive(Serialize, Deserialize)] struct CachedUser {     id: String,     name: String,     email: String,     // Skip: password, picture, preferences }   Solution 3C: Limit cache size  # Redis: Set maximum memory redis-cli CONFIG SET maxmemory 2gb redis-cli CONFIG SET maxmemory-policy allkeys-lru  # Memcached: Set maximum memory at startup memcached -m 2048  # 2GB   Solution 3D: Monitor key count  // Alert if key count grows unbounded pub fn check_cache_health(metrics: &amp;Metrics) {     let key_count = get_redis_dbsize();          if key_count &gt; ALERT_THRESHOLD {         alert!(\"Cache size growing: {} keys\", key_count);         // Investigate: Are keys not expiring?     } }     Issue 4: Serialization Errors   Symptoms:     “Serialization failed” errors   “Version mismatch” errors   “Invalid magic header” errors   Some requests fail, others work   Possible Causes:     Entity type changed (schema mismatch)   Corrupted cache entry   Type contains unsupported fields (e.g., Decimal)   Different serialization formats   Diagnosis Steps   Step 1: Check error logs  grep -i \"serialization\" app.log | head -20 # Output: # 2024-01-15 10:23:45 WARN: Serialization failed for user:123: Unsupported type   Step 2: Identify affected entities  grep -i \"serialization\" app.log | grep -o \"user:[0-9]*\" | sort | uniq # Output: user:123, user:456, user:789 # All from same key? Different ones? Pattern?   Step 3: Check entity definition  // Look for unsupported types #[derive(Serialize, Deserialize)] struct User {     id: String,     name: String,     balance: rust_decimal::Decimal,  // ❌ Not supported by Postcard! }  // Or type changed // Version 1: // struct User { id: String, name: String } // Version 2: // struct User { id: String, name: String, email: String }  // Added field!   Solutions   Solution 4A: Clear affected entries  # Redis: Delete specific key redis-cli DEL \"user:123\"  # Or delete all of a type redis-cli KEYS \"user:*\" | xargs redis-cli DEL  # Memcached echo \"delete user:123\" | nc localhost 11211   Solution 4B: Replace Decimal with i64  // Before: Uses Decimal #[derive(Serialize, Deserialize)] struct CachedProduct {     id: String,     price: rust_decimal::Decimal,  // ❌ Not serializable }  // After: Use integer cents #[derive(Serialize, Deserialize)] struct CachedProduct {     id: String,     price_cents: i64,  // ✅ Serializable }  impl CachedProduct {     pub fn price(&amp;self) -&gt; f64 {         self.price_cents as f64 / 0.9.0     } }   Solution 4C: Use cache-specific DTO  // Database type (with Decimal) #[derive(sqlx::FromRow)] struct ProductRow {     id: String,     price: rust_decimal::Decimal, }  // Cache type (with i64) #[derive(Serialize, Deserialize)] struct CachedProduct {     id: String,     price_cents: i64, }  impl From&lt;ProductRow&gt; for CachedProduct {     fn from(row: ProductRow) -&gt; Self {         CachedProduct {             id: row.id,             price_cents: (row.price * 100).to_i64().unwrap_or(0),         }     } }     Logging Setup   Configure Log Levels   // In your main.rs or lib.rs use tracing_subscriber;  fn main() {     // Development: DEBUG level     #[cfg(debug_assertions)]     {         tracing_subscriber::fmt()             .with_max_level(Level::DEBUG)             .init();     }      // Production: INFO level (less verbose)     #[cfg(not(debug_assertions))]     {         tracing_subscriber::fmt()             .with_max_level(Level::INFO)             .init();     } }   Environment Variable Configuration   # Enable debug logging RUST_LOG=cache=debug cargo run  # Cache-kit only RUST_LOG=cache_kit=trace cargo run  # Everything RUST_LOG=debug cargo run   Structured Logging   use tracing::{info, warn, error, debug};  // ✅ Good: Structured, searchable logs info!(     user_id = %user_id,     cache_hit = hit,     latency_ms = latency.as_millis(),     \"Cache operation completed\" );  // ❌ Bad: Unstructured, hard to parse info!(\"Cache operation for user {} completed in {:?}ms\", user_id, latency);   Common Log Patterns   ✅ Cache hit (expected):     info!(\"Cache hit for user:{}\", user_id);  ✅ Cache miss (expected):     debug!(\"Cache miss for user:{}, fetching from DB\", user_id);  ✅ Backend error (expected but needs handling):     warn!(\"Cache backend unavailable: {}, using fallback\", error);  ❌ Serialization error (unexpected):     error!(\"Serialization error for user:{}: {}\", user_id, error);  ❌ Pool exhausted (capacity issue):     error!(\"Connection pool exhausted: {} active, {} waiting\", active, waiting);     Health Checks   Backend Health Check Implementation   pub async fn check_cache_health(     cache: &amp;mut CacheExpander&lt;RedisBackend&gt;, ) -&gt; Result&lt;HealthStatus&gt; {     let start = Instant::now();      match cache.health_check().await {         Ok(true) =&gt; {             let latency = start.elapsed();             info!(\"Cache healthy, latency: {:?}\", latency);                          if latency &gt; Duration::from_millis(100) {                 warn!(\"Cache is slow: {:?}\", latency);                 Ok(HealthStatus::Degraded)             } else {                 Ok(HealthStatus::Healthy)             }         }         Ok(false) =&gt; {             error!(\"Cache health check failed\");             Ok(HealthStatus::Unhealthy)         }         Err(e) =&gt; {             error!(\"Health check error: {}\", e);             Err(e)         }     } }   Periodic Health Monitoring   pub async fn monitor_cache_health(     cache: Arc&lt;Mutex&lt;CacheExpander&lt;RedisBackend&gt;&gt;&gt;, ) {     tokio::spawn(async move {         loop {             tokio::time::sleep(Duration::from_secs(30)).await;              let mut cache = cache.lock().await;             match check_cache_health(&amp;mut cache).await {                 Ok(HealthStatus::Healthy) =&gt; {                     debug!(\"Cache health: OK\");                 }                 Ok(HealthStatus::Degraded) =&gt; {                     warn!(\"Cache health: DEGRADED (slow responses)\");                 }                 Ok(HealthStatus::Unhealthy) =&gt; {                     error!(\"Cache health: DOWN\");                 }                 Err(e) =&gt; {                     error!(\"Health check failed: {}\", e);                 }             }         }     }); }   Expose Health Endpoint   // Axum example async fn health_check(     State(cache): State&lt;Arc&lt;Mutex&lt;CacheExpander&lt;RedisBackend&gt;&gt;&gt;&gt;, ) -&gt; impl IntoResponse {     let mut cache = cache.lock().await;      match cache.health_check().await {         Ok(true) =&gt; Json(serde_json::json!({             \"status\": \"healthy\",             \"cache\": \"ready\"         })),         _ =&gt; (             StatusCode::SERVICE_UNAVAILABLE,             Json(serde_json::json!({                 \"status\": \"unhealthy\",                 \"cache\": \"unavailable\"             })),         ),     } }  // In router app.route(\"/health\", get(health_check))     Production Troubleshooting Checklist   Use this checklist when issues occur:   Cache Issues      Is Redis/Memcached running? (redis-cli ping)   Is network connectivity OK? (nc -zv localhost 6379)   Are connection pool metrics available?   What’s the hit rate? (&lt; 20% = investigate TTL/keys)   Are there serialization errors? (check entity types)   Is memory usage growing unbounded? (check TTL)   Are cache keys deterministic? (check key generation)   Network Issues      Is backend reachable? (netstat or ss)   What’s the latency? (redis-cli --latency)   Are there packet drops? (netstat -s)   Is there network congestion?   Did firewall rules change?   Application Issues      Are error logs being generated? (grep error app.log)   Is the cache fallback code working?   Are metrics being exported?   Is the database under load?   Did schema change recently?   System Issues      CPU usage normal? (top)   Memory usage normal? (free -h)   Disk space available? (df -h)   System load average? (uptime)   Are there OOM killings? (dmesg | tail)     Getting Help   If you can’t resolve the issue:      Gather diagnostics:     redis-cli INFO server &gt; redis_info.txt redis-cli DBSIZE &gt; redis_size.txt top -b -n 1 &gt; system_info.txt grep -i cache app.log &gt; cache_logs.txt           Check recent changes:            Code deploy?       Config change?       Infrastructure change?       Data volume increase?           Enable debug logging:     RUST_LOG=cache=debug,cache_kit=trace cargo run           Open an issue: https://github.com/megamsys/cache-kit.rs/issues       Error Handling Best Practices   DO ✅   1. Handle cache errors gracefully  // Good: Don't crash on cache errors match cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh) {     Ok(_) =&gt; {         // Use cached data from feeder         Some(feeder.user)     }     Err(e) =&gt; {         // Cache failed - fallback to database         eprintln!(\"Cache error: {}, using fallback\", e);         repo.fetch_by_id(&amp;user_id).ok().flatten()     } }   2. Log cache errors separately  // Good: Log with context match cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh) {     Ok(_) =&gt; info!(\"Cache operation successful\"),     Err(e) =&gt; error!(\"Cache error: {}\", e),  // Separate from app logic }   3. Use Result types properly  // Good: Propagate errors with context pub fn get_user(id: String) -&gt; Result&lt;User&gt; {     // Use ? operator     cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)?;     Ok(feeder.user.ok_or(Error::NotFound)?) }  // Bad: Swallowing errors silently pub fn get_user(id: String) -&gt; Option&lt;User&gt; {     cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh).ok();     feeder.user }   4. Distinguish cache errors from application errors  // Good: Different handling for different error types match cache_result {     Err(Error::BackendError(_)) =&gt; fallback_to_database(),     Err(Error::SerializationError(_)) =&gt; invalidate_and_refetch(),     Err(Error::TimeoutError(_)) =&gt; use_stale_data(),     Ok(_) =&gt; use_fresh_data(), }  // Bad: Treat all errors the same if cache_result.is_err() {     panic!(\"Cache failed!\"); }   5. Test error paths thoroughly  #[test] fn test_cache_error_fallback() {     let backend = FailingBackend::new();  // Always fails     let mut cache = CacheExpander::new(backend);          let result = get_user_with_fallback(&amp;mut cache, &amp;repo, \"123\".to_string());          assert!(result.is_ok());  // Still works despite backend failure     assert_eq!(result.unwrap(), Some(expected_user)); }   DON’T ❌   1. Panic on cache errors  // Bad: Crashes the service let result = cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)     .expect(\"Cache must work!\");  // 💥 Production incident  // Good: Handle gracefully let _ = cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)     .map_err(|e| eprintln!(\"Cache error: {}\", e));   2. Expose cache errors to users  // Bad: User sees internal error async fn get_user(id: String) -&gt; Result&lt;User, String&gt; {     let result = cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)?;     Ok(feeder.user.ok_or_else(|| \"Cache error: serialization failed\".to_string())?) }  // Good: Return user-friendly error async fn get_user(id: String) -&gt; Result&lt;User&gt; {     let result = cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh)         .map_err(|e| {             error!(\"Cache error: {}\", e);  // Log internally             StatusCode::INTERNAL_SERVER_ERROR  // Return generic error         })?;     Ok(feeder.user.ok_or(Error::NotFound)?) }   3. Ignore all errors equally  // Bad: All errors treated the same if cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh).is_ok() {     return feeder.user; } return database_fallback();  // Good: Different handling per error match cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Refresh) {     Ok(_) =&gt; feeder.user,     Err(Error::BackendError(_)) =&gt; database_fallback(),  // Try DB     Err(Error::SerializationError(_)) =&gt; {  // Clear and refetch         cache.with(&amp;mut feeder, &amp;repo, CacheStrategy::Invalidate).ok();         database_fallback()     }     Err(e) =&gt; {         eprintln!(\"Unexpected error: {}\", e);         None     } }   4. Cascade failures across services  // Bad: Cache failure blocks all dependent services service1 depends on cache ❌ service2 depends on service1 ❌ service3 depends on service2 ❌ // If cache fails, everything fails  // Good: Each service has its own fallback service1 has fallback to DB ✅ service2 has fallback to API ✅ service3 has fallback to queue ✅ // If any fails, others continue   5. Log sensitive data in error messages  // Bad: Logs contain user data Err(e) =&gt; {     error!(\"Failed to cache user: {:?}\", user);  // 🔓 Sensitive data }  // Good: Only log IDs Err(e) =&gt; {     error!(\"Failed to cache user {}: {}\", user.id, e);  // ✅ Safe }     Next Steps      Set up Monitoring and metrics to detect issues early   Check Performance tuning for optimization     See Also      Monitoring Guide — Set up metrics and alerting   Cache Backends — Backend-specific configuration  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/guides/troubleshooting/",
        "teaser": null
      },{
        "title": "Installation & Configuration",
        "excerpt":"  Prerequisites      Rust: 1.75 or later   Tokio: 1.41 or later (async runtime)     Installation   Add cache-kit to your Cargo.toml:   [dependencies] cache-kit = { version = \"0.9\" } serde = { version = \"1.0\", features = [\"derive\"] } tokio = { version = \"1.41\", features = [\"rt\", \"sync\", \"macros\"] }   Feature Flags   cache-kit uses feature flags to enable optional backends:                  Feature       Description       Default                       inmemory       In-memory cache backend       ✅ Enabled                 redis       Redis backend with connection pooling       ❌ Optional                 memcached       Memcached backend       ❌ Optional                 all       Enable all backends       ❌ Optional           Basic Installation (InMemory Only)   [dependencies] cache-kit = { version = \"0.9\" }   This provides the InMemory backend, perfect for:      Development   Testing   Single-instance services   Redis Backend   [dependencies] cache-kit = { version = \"0.9\", features = [\"redis\"] }   Enables production-grade Redis caching with connection pooling.   Memcached Backend   [dependencies] cache-kit = { version = \"0.9\", features = [\"memcached\"] }   Enables Memcached backend for high-performance distributed caching.   All Backends   [dependencies] cache-kit = { version = \"0.9\", features = [\"all\"] }   Enables all available backends. Useful for:      Testing multiple backends   Switching backends based on environment   Benchmarking comparisons     Minimal Configuration   cache-kit requires minimal configuration. Here’s a complete working example:   use cache_kit::{     CacheEntity, CacheFeed, DataRepository, CacheService,     backend::InMemoryBackend,     strategy::CacheStrategy, }; use serde::{Deserialize, Serialize};  #[derive(Clone, Serialize, Deserialize)] struct User {     id: String,     name: String, }  impl CacheEntity for User {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key { self.id.clone() }     fn cache_prefix() -&gt; &amp;'static str { \"user\" } }  struct UserFeeder {     id: String,     user: Option&lt;User&gt;, }  impl CacheFeed&lt;User&gt; for UserFeeder {     fn entity_id(&amp;mut self) -&gt; String { self.id.clone() }     fn feed(&amp;mut self, entity: Option&lt;User&gt;) { self.user = entity; } }  struct UserRepository;  impl DataRepository&lt;User&gt; for UserRepository {     async fn fetch_by_id(&amp;self, id: &amp;String) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         Ok(Some(User {             id: id.clone(),             name: \"Alice\".to_string(),         }))     } }  #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {     let backend = InMemoryBackend::new();     let cache = CacheService::new(backend);     let repository = UserRepository;      let mut feeder = UserFeeder {         id: \"user_001\".to_string(),         user: None,     };      cache.execute(&amp;mut feeder, &amp;repository, CacheStrategy::Refresh).await?;      println!(\"User: {:?}\", feeder.user);     Ok(()) }     Backend Configuration   ⚠️ Production Backend Requirement   InMemory backend is for development/testing only. Use Redis or Memcached for production deployments.     InMemory Backend   No configuration required:   use cache_kit::{CacheService, backend::InMemoryBackend};  let cache = CacheService::new(InMemoryBackend::new());   The InMemory backend uses DashMap internally, providing:      Lock-free concurrent HashMap   Thread-safe operations   Zero external dependencies   Redis Backend   use cache_kit::{CacheService, backend::{RedisBackend, RedisConfig}};  let config = RedisConfig {     url: \"redis://localhost:6379\".to_string(),     max_connections: 10,     min_connections: 2,     connection_timeout_secs: 5, };  let cache = CacheService::new(RedisBackend::new(config)?);   Redis Configuration Options                  Field       Type       Default       Description                       url       String       Required       Redis connection URL                 max_connections       usize       10       Maximum connection pool size                 min_connections       usize       2       Minimum idle connections                 connection_timeout_secs       u64       5       Connection timeout in seconds           Redis URL Formats   // Local Redis \"redis://localhost:6379\"  // Remote Redis with password \"redis://:password@example.com:6379\"  // Redis with specific database \"redis://localhost:6379/1\"  // TLS connection \"rediss://example.com:6379\"   Environment-Based Configuration   use std::env;  let redis_url = env::var(\"REDIS_URL\")     .unwrap_or_else(|_| \"redis://localhost:6379\".to_string());  let config = RedisConfig {     url: redis_url,     ..Default::default() };  let backend = RedisBackend::new(config)?;   Memcached Backend   use cache_kit::{CacheService, backend::{MemcachedBackend, MemcachedConfig}};  let config = MemcachedConfig {     servers: vec![\"localhost:11211\".to_string()],     max_connections: 10,     min_connections: 2, };  let cache = CacheService::new(MemcachedBackend::new(config)?);   Memcached Configuration Options                  Field       Type       Default       Description                       servers       Vec&lt;String&gt;       Required       Memcached server addresses                 max_connections       usize       10       Maximum connection pool size                 min_connections       usize       2       Minimum idle connections           Multiple Memcached Servers   let config = MemcachedConfig {     servers: vec![         \"memcached-01:11211\".to_string(),         \"memcached-02:11211\".to_string(),         \"memcached-03:11211\".to_string(),     ],     max_connections: 20,     min_connections: 5, };  let backend = MemcachedBackend::new(config)?;     TTL Configuration   Configure time-to-live (TTL) for cached entries:   Global TTL   use std::time::Duration; use cache_kit::{CacheService, observability::TtlPolicy, backend::InMemoryBackend};  let cache = CacheService::new(InMemoryBackend::new()); // Note: TTL configuration via CacheService is set through backend configuration   No TTL (Cache Forever)   use cache_kit::{CacheService, backend::InMemoryBackend};  // Don't set TTL - cached entries never expire let cache = CacheService::new(InMemoryBackend::new());   Note: “Cache forever” is not recommended for production. Always set appropriate TTLs based on your data freshness requirements.     Environment-Based Configuration   Create a configuration module for your application:   use cache_kit::{CacheService, backend::{InMemoryBackend, RedisBackend, RedisConfig}}; use std::env;  pub enum Environment {     Development,     Production, }  impl Environment {     pub fn from_env() -&gt; Self {         match env::var(\"ENV\").as_deref() {             Ok(\"production\") =&gt; Environment::Production,             _ =&gt; Environment::Development,         }     } }  pub fn create_cache_service() -&gt; CacheService&lt;impl cache_kit::backend::CacheBackend&gt; {     match Environment::from_env() {         Environment::Development =&gt; {             CacheService::new(InMemoryBackend::new())         }         Environment::Production =&gt; {             let redis_url = env::var(\"REDIS_URL\")                 .expect(\"REDIS_URL must be set in production\");              let config = RedisConfig {                 url: redis_url,                 max_connections: 20,                 min_connections: 5,                 connection_timeout_secs: 10,             };              CacheService::new(RedisBackend::new(config).expect(\"Failed to connect to Redis\"))         }     } }   Usage:   #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {     let cache = create_cache_service();      // Your application logic     Ok(()) }     Docker Compose for Development   Use Docker Compose to run Redis and Memcached locally:   version: \"3.8\"  services:   redis:     image: redis:7-alpine     ports:       - \"6379:6379\"     volumes:       - redis_data:/data     command: redis-server --appendonly yes    memcached:     image: memcached:1.6-alpine     ports:       - \"11211:11211\"     command: memcached -m 64  volumes:   redis_data:   Start services:   docker-compose up -d   Test connections:   # Redis redis-cli ping  # Should return: PONG  # Memcached echo \"stats\" | nc localhost 11211  # Should return stats     Testing Configuration   For unit and integration tests, use the InMemory backend:   #[cfg(test)] mod tests {     use super::*;     use cache_kit::backend::InMemoryBackend;      #[tokio::test]     async fn test_user_caching() {         // Use InMemory backend for tests (no external dependencies)         let backend = InMemoryBackend::new();         let mut expander = CacheExpander::new(backend);          // Your test logic     } }   Test Isolation   Each test should create its own backend instance to avoid interference:   #[tokio::test] async fn test_cache_hit() {     let backend = InMemoryBackend::new();  // Fresh instance     let mut expander = CacheExpander::new(backend);     // Test logic }  #[tokio::test] async fn test_cache_miss() {     let backend = InMemoryBackend::new();  // Separate instance     let mut expander = CacheExpander::new(backend);     // Test logic }     Production Checklist   Before deploying cache-kit to production:      Backend selected: Redis or Memcached for production   Connection pooling configured: Set appropriate max_connections   TTL policies defined: Set TTLs based on data freshness requirements   Error handling implemented: Handle cache failures gracefully   Monitoring enabled: Track cache hit/miss rates   Environment variables set: REDIS_URL or MEMCACHED_SERVERS   Fallback strategy: Application works if cache is unavailable   Load tested: Verify performance under expected load     Common Configuration Patterns   Pattern 1: Shared Cache Across Services   use cache_kit::{CacheService, backend::{RedisBackend, RedisConfig}};  let cache = CacheService::new(RedisBackend::new(config)?);  // CacheService is Clone - easily share across services let user_service = UserService::new(cache.clone()); let product_service = ProductService::new(cache.clone());   Pattern 2: Multiple Cache Backends   use cache_kit::{CacheService, backend::{RedisBackend, RedisConfig}};  // User cache let user_backend = RedisBackend::new(user_config)?; let user_cache = CacheService::new(user_backend);  // Product cache let product_backend = RedisBackend::new(product_config)?; let product_cache = CacheService::new(product_backend);   Pattern 3: Read-Through Cache   use cache_kit::{CacheService, DataRepository, CacheFeed, strategy::CacheStrategy, backend::RedisBackend};  pub struct CachedRepository&lt;R&gt; {     repository: R,     cache: CacheService&lt;RedisBackend&gt;, }  impl&lt;R: DataRepository&lt;User&gt;&gt; CachedRepository&lt;R&gt; {     pub async fn get_user(&amp;self, id: &amp;str) -&gt; cache_kit::Result&lt;Option&lt;User&gt;&gt; {         let mut feeder = UserFeeder {             id: id.to_string(),             user: None,         };          // Always use Refresh strategy for read-through         self.cache.execute(&amp;mut feeder, &amp;self.repository, CacheStrategy::Refresh).await?;          Ok(feeder.user)     } }     Next Steps      Learn about Database &amp; ORM compatibility   Explore Cache backend options in detail   Review Serialization formats   See the Actix + SQLx example  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/installation/",
        "teaser": null
      },{
        "title": "Reference",
        "excerpt":"Reference Documentation   Technical reference materials and project status information.   Project Information      Version: 0.9.0   Status: Production-ready ✅   Test Coverage: 193 passing tests (~68% coverage)   Compilation: Zero errors, zero warnings  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/reference/index/",
        "teaser": null
      },{
        "title": "Serialization Support",
        "excerpt":"  ⚠️ Critical Limitation   Decimal types (rust_decimal::Decimal, bigdecimal::BigDecimal) are NOT supported by Postcard serialization.   If your entities use Decimal fields (common in financial apps), you MUST convert to String or i64 before caching. See Decimal Workarounds below.     Serialization as a First-Class Concern   cache-kit treats serialization as a pluggable, first-class concern.   Serialization determines:     Storage format in the cache backend   Performance characteristics (speed, size)   Type support (which Rust types can be cached)   Interoperability (can other languages read the cache?)     Supported Formats   Tier-1: Postcard (Recommended)   Postcard is the primary recommended serialization format for cache-kit.                  Feature       Postcard                       Performance       ⚡ Very fast (10-15x faster than JSON)                 Size       📦 Compact (40-50% smaller than JSON)                 Type safety       ✅ Strong Rust type preservation                 Determinism       ✅ Same input → same output                 Language support       ❌ Rust-only                 Decimal support       ❌ No (see limitations below)           Why Postcard?      Optimized for Rust — Zero-copy deserialization where possible   No schema evolution — Simple, explicit versioning   Minimal overhead — Field order matters, no field names stored   Fast — Designed for embedded and performance-critical systems   Installation   Postcard is included by default:   [dependencies] cache-kit = \"0.9\"   Usage   No explicit configuration needed — cache-kit uses Postcard automatically:   use cache_kit::CacheEntity; use serde::{Deserialize, Serialize};  #[derive(Clone, Serialize, Deserialize)] struct User {     id: String,     name: String,     age: u32, }  impl CacheEntity for User {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key { self.id.clone() }     fn cache_prefix() -&gt; &amp;'static str { \"user\" } }  // Serialization to Postcard happens automatically     Tier-2: MessagePack (Planned)   MessagePack will be available as an alternative serialization format.                  Feature       MessagePack (Planned)                       Performance       ⚡ Fast (4-6x faster than JSON)                 Size       📦 Compact (50% smaller than JSON)                 Type safety       ⚠️ Partial                 Determinism       ⚠️ Partial (field order varies)                 Language support       ✅ Many languages                 Decimal support       ⚠️ Depends on implementation           Community contributions welcome! Help us add MessagePack support.     Serialization Characteristics   Postcard: Binary, Deterministic   use serde::{Deserialize, Serialize};  #[derive(Serialize, Deserialize)] struct Product {     id: u64,          // 8 bytes (compact)     name: String,     // length-prefixed     price: f64,       // 8 bytes (IEEE 754) }  // Serialized format (example): // [id: 8 bytes][name_len: varint][name: UTF-8 bytes][price: 8 bytes]   Key property: Serializing the same value twice produces identical bytes.   let product1 = Product { id: 123, name: \"Widget\".to_string(), price: 99.99 }; let product2 = Product { id: 123, name: \"Widget\".to_string(), price: 99.99 };  let bytes1 = postcard::to_allocvec(&amp;product1)?; let bytes2 = postcard::to_allocvec(&amp;product2)?;  assert_eq!(bytes1, bytes2);  // ✅ Always true   This enables:     Reliable cache keys based on content   Deduplication in distributed caches   Reproducible testing     Known Limitations   Decimal Types Not Supported   Postcard (and many binary formats) do not support arbitrary-precision decimal types out of the box.   Affected types:     rust_decimal::Decimal   bigdecimal::BigDecimal   Database NUMERIC / DECIMAL columns   Why This Limitation Exists   Binary formats like Postcard serialize types based on their in-memory representation. Decimal types have complex internal structures that don’t map cleanly to portable binary formats.   Workaround Strategies   Strategy 1: Convert to Supported Primitives   Store monetary values as integer cents instead of decimal dollars:   use serde::{Deserialize, Serialize};  #[derive(Clone, Serialize, Deserialize)] struct Product {     id: String,     name: String,     price_cents: i64,  // ✅ Store $99.99 as 9999 cents }  impl Product {     pub fn price_dollars(&amp;self) -&gt; f64 {         self.price_cents as f64 / 0.9.0     }      pub fn set_price_dollars(&amp;mut self, dollars: f64) {         self.price_cents = (dollars * 0.9.0).round() as i64;     } }   Pros:     ✅ No precision loss for monetary values   ✅ Fast serialization   ✅ Compact storage   Cons:     ❌ Manual conversion needed   ❌ Limited to representable range of i64   Strategy 2: Cache-Specific DTOs   Create separate types for caching:   // Database model (with Decimal) #[derive(sqlx::FromRow)] struct ProductRow {     id: String,     name: String,     price: rust_decimal::Decimal,  // Database DECIMAL type }  // Cache model (with supported types) #[derive(Clone, Serialize, Deserialize)] struct CachedProduct {     id: String,     name: String,     price_cents: i64,  // Converted from Decimal }  impl CacheEntity for CachedProduct {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key { self.id.clone() }     fn cache_prefix() -&gt; &amp;'static str { \"product\" } }  impl From&lt;ProductRow&gt; for CachedProduct {     fn from(row: ProductRow) -&gt; Self {         CachedProduct {             id: row.id,             name: row.name,             price_cents: (row.price * rust_decimal::Decimal::from(100))                 .to_i64()                 .unwrap_or(0),         }     } }   Pros:     ✅ Clean separation of concerns   ✅ Database can use appropriate types   ✅ Cache uses efficient types   Cons:     ❌ Requires type conversion   ❌ More boilerplate   Strategy 3: String Representation   Store decimals as strings (not recommended for performance):   #[derive(Clone, Serialize, Deserialize)] struct Product {     id: String,     name: String,     price: String,  // \"99.99\" as string }   Pros:     ✅ No precision loss   ✅ Preserves exact decimal representation   Cons:     ❌ Slower serialization   ❌ Larger storage footprint   ❌ Manual parsing required   Strategy 4: Use MessagePack (Future)   When MessagePack support is added, you may have more flexibility for decimal types.   Community contributions welcome!     Serialization Best Practices   DO   ✅ Use primitive types where possible (i64, f64, String) ✅ Convert decimals to integers (cents) for monetary values ✅ Create cache-specific DTOs if needed ✅ Document conversion logic clearly ✅ Test roundtrip serialization   DON’T   ❌ Assume all Rust types are serializable ❌ Mix database types with cache types without conversion ❌ Ignore serialization errors ❌ Use unwrap() on deserialization ❌ Store sensitive data without encryption     Custom Serialization   If you need custom serialization for specific types, implement serde traits:   use serde::{Deserialize, Deserializer, Serialize, Serializer}; use rust_decimal::Decimal;  #[derive(Clone)] struct CustomProduct {     id: String,     name: String,     price: Decimal, }  impl Serialize for CustomProduct {     fn serialize&lt;S&gt;(&amp;self, serializer: S) -&gt; Result&lt;S::Ok, S::Error&gt;     where         S: Serializer,     {         use serde::ser::SerializeStruct;          let mut state = serializer.serialize_struct(\"CustomProduct\", 3)?;         state.serialize_field(\"id\", &amp;self.id)?;         state.serialize_field(\"name\", &amp;self.name)?;          // Convert Decimal to i64 cents         let price_cents = (self.price * Decimal::from(100))             .to_i64()             .ok_or_else(|| serde::ser::Error::custom(\"Price out of range\"))?;         state.serialize_field(\"price_cents\", &amp;price_cents)?;          state.end()     } }  impl&lt;'de&gt; Deserialize&lt;'de&gt; for CustomProduct {     fn deserialize&lt;D&gt;(deserializer: D) -&gt; Result&lt;Self, D::Error&gt;     where         D: Deserializer&lt;'de&gt;,     {         #[derive(Deserialize)]         struct Helper {             id: String,             name: String,             price_cents: i64,         }          let helper = Helper::deserialize(deserializer)?;          Ok(CustomProduct {             id: helper.id,             name: helper.name,             price: Decimal::from(helper.price_cents) / Decimal::from(100),         })     } }     Versioning and Schema Evolution   cache-kit uses explicit versioning for cached data.   Current Approach   cache-kit wraps all cached entries in a versioned envelope:   [MAGIC (4 bytes)] [VERSION (4 bytes)] [POSTCARD PAYLOAD]      MAGIC: b\"CKIT\" — Identifies cache-kit entries   VERSION: u32 — Schema version number   PAYLOAD: Postcard-serialized entity   Version Mismatches   When the schema version changes:      Old entries rejected — Cannot be deserialized   Cache miss triggered — Fetch from database   New entry cached — With updated version   No migration — Cache naturally repopulates with new schema.   Handling Schema Changes   When you modify an entity structure:   // Version 1 struct User {     id: String,     name: String, }  // Version 2 (added field) struct User {     id: String,     name: String,     email: Option&lt;String&gt;,  // New field }   Action required:     Bump schema version in cache-kit configuration   Deploy new code   Old cache entries will be invalidated automatically   New entries cached with updated schema     Performance Characteristics   Postcard Performance   Based on typical workloads:                  Operation       Time       Throughput                       Serialize (1KB entity)       50-100 ns       10-20M ops/sec                 Deserialize (1KB entity)       60-120 ns       8-16M ops/sec           Comparison with JSON:                  Metric       JSON       Postcard       Improvement                       Serialize       1.2 µs       80 ns       15x faster                 Deserialize       1.5 µs       100 ns       15x faster                 Size (1KB entity)       158 bytes       95 bytes       40% smaller             Example: Complete Serialization Flow   use cache_kit::{CacheEntity, CacheExpander}; use cache_kit::backend::InMemoryBackend; use serde::{Deserialize, Serialize};  // 1. Define entity (automatically uses Postcard) #[derive(Clone, Serialize, Deserialize, Debug)] struct User {     id: String,     name: String,     age: u32, }  impl CacheEntity for User {     type Key = String;     fn cache_key(&amp;self) -&gt; Self::Key { self.id.clone() }     fn cache_prefix() -&gt; &amp;'static str { \"user\" } }  fn main() -&gt; cache_kit::Result&lt;()&gt; {     let backend = InMemoryBackend::new();     let mut expander = CacheExpander::new(backend);      let user = User {         id: \"user_001\".to_string(),         name: \"Alice\".to_string(),         age: 30,     };      // Serialization happens automatically     // [MAGIC][VERSION][Postcard bytes]     expander.set(&amp;user, None)?;      // Deserialization happens automatically     let cached: Option&lt;User&gt; = expander.get(&amp;\"user_001\".to_string())?;      println!(\"Cached user: {:?}\", cached);      Ok(()) }     Troubleshooting   Error: “Serialization failed”   Cause: Entity contains unsupported types (e.g., Decimal)   Solution: Convert to supported primitives or use cache-specific DTOs   Error: “Version mismatch”   Cause: Cached entry has different schema version   Solution: This is expected after schema changes. Entry will be invalidated and refetched.   Error: “Invalid magic header”   Cause: Cache entry is corrupted or not created by cache-kit   Solution: Clear the cache key manually or let it expire     Next Steps      Learn about Cache backend options   Review Design principles   Explore the Actix + SQLx reference implementation   Contribute MessagePack support! See CONTRIBUTING.md  ","categories": [],
        "tags": [],
        "url": "/cache-kit.rs/serialization/",
        "teaser": null
      },]
